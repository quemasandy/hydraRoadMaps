# Ejercicio 06: RegresiÃ³n Lineal

**Objetivo:** Implementar algoritmos de regresiÃ³n lineal desde cero, tanto con la ecuaciÃ³n normal como con gradient descent, para predecir valores continuos.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© es la RegresiÃ³n Lineal?

La regresiÃ³n lineal es uno de los algoritmos mÃ¡s fundamentales en Machine Learning. Se usa para:
- **Predecir valores continuos**: Precio de casas, temperatura, ventas
- **Entender relaciones**: CÃ³mo las features afectan el target
- **Baseline models**: Punto de partida antes de modelos complejos
- **Interpretabilidad**: Los coeficientes tienen significado claro

### El Modelo

Queremos encontrar una funciÃ³n lineal que relacione las features `X` con el target `y`:

```
Å· = Î¸0 + Î¸1Ã—x1 + Î¸2Ã—x2 + ... + Î¸nÃ—xn
```

En notaciÃ³n matricial:
```
Å· = X Ã— Î¸
```

Donde:
- `Å·` (y-hat): PredicciÃ³n
- `X`: Matriz de features (m Ã— n)
- `Î¸` (theta): Vector de parÃ¡metros (n)
- `Î¸0`: Intercepto (bias)
- `Î¸1...Î¸n`: Pesos de cada feature

### FunciÃ³n de Costo: MSE (Mean Squared Error)

Queremos minimizar la diferencia entre predicciones y valores reales:

```
J(Î¸) = (1/2m) Ã— Î£(Å·i - yi)Â²
     = (1/2m) Ã— Î£(Î¸0 + Î¸1Ã—xi1 + ... + Î¸nÃ—xin - yi)Â²
```

Donde:
- `m`: NÃºmero de ejemplos
- El factor `1/2` facilita las derivadas

**Â¿Por quÃ© MSE?**
- Penaliza errores grandes (cuadrÃ¡tico)
- Diferenciable (permite gradient descent)
- Tiene soluciÃ³n analÃ­tica (ecuaciÃ³n normal)

### MÃ©todos de SoluciÃ³n

#### 1. **EcuaciÃ³n Normal (Normal Equation)**

SoluciÃ³n analÃ­tica directa sin iteraciones:

```
Î¸ = (X^T Ã— X)^-1 Ã— X^T Ã— y
```

**Ventajas:**
- No requiere elegir learning rate
- No requiere iteraciones
- Da la soluciÃ³n Ã³ptima directamente

**Desventajas:**
- Muy lento con muchas features (O(nÂ³) por la inversiÃ³n)
- No funciona si X^T Ã— X es singular (no invertible)
- No escala bien (> 10,000 features)

**CuÃ¡ndo usar:**
- Pocas features (< 10,000)
- Quieres la soluciÃ³n exacta
- No necesitas entrenar iterativamente

#### 2. **Gradient Descent**

SoluciÃ³n iterativa que usa optimizaciÃ³n:

```
Î¸ = Î¸ - Î± Ã— âˆ‡J(Î¸)
```

**Gradiente del MSE:**
```
âˆ‚J/âˆ‚Î¸j = (1/m) Ã— Î£(Å·i - yi) Ã— xij
âˆ‡J(Î¸) = (1/m) Ã— X^T Ã— (XÃ—Î¸ - y)
```

**Ventajas:**
- Escala bien con muchas features
- Funciona con datasets grandes
- Puede usar mini-batch / SGD

**Desventajas:**
- Requiere elegir learning rate
- Requiere mÃºltiples iteraciones
- Puede requerir normalizaciÃ³n de features

**CuÃ¡ndo usar:**
- Muchas features (> 10,000)
- Datasets grandes (> 1,000,000 ejemplos)
- Necesitas actualizar el modelo incrementalmente

### MÃ©tricas de EvaluaciÃ³n

#### 1. **MSE (Mean Squared Error)**

```
MSE = (1/m) Ã— Î£(Å·i - yi)Â²
```

**CaracterÃ­sticas:**
- Unidades al cuadrado del target
- Penaliza errores grandes
- Sensible a outliers

#### 2. **RMSE (Root Mean Squared Error)**

```
RMSE = âˆšMSE
```

**CaracterÃ­sticas:**
- Mismas unidades que el target
- MÃ¡s interpretable que MSE
- EstÃ¡ndar en competencias de ML

#### 3. **MAE (Mean Absolute Error)**

```
MAE = (1/m) Ã— Î£|Å·i - yi|
```

**CaracterÃ­sticas:**
- Menos sensible a outliers
- Lineal, no cuadrÃ¡tico
- Mismas unidades que el target

#### 4. **RÂ² (Coefficient of Determination)**

```
RÂ² = 1 - (SS_res / SS_tot)
SS_res = Î£(yi - Å·i)Â²  (suma de residuos)
SS_tot = Î£(yi - È³)Â²   (varianza total)
```

**InterpretaciÃ³n:**
- `RÂ² = 1`: Modelo perfecto
- `RÂ² = 0`: Modelo tan bueno como la media
- `RÂ² < 0`: Modelo peor que la media

**Ventajas:**
- Normalizado (0 a 1)
- Independiente de escala
- FÃ¡cil de interpretar (% de varianza explicada)

### Feature Scaling

Normalizar features acelera convergencia:

#### 1. **Normalization (Min-Max Scaling)**

```
x_norm = (x - min) / (max - min)
```

Rango: [0, 1]

#### 2. **Standardization (Z-score)**

```
x_std = (x - Î¼) / Ïƒ
```

Rango: aprox. [-3, 3]

**Â¿CuÃ¡l usar?**
- **Normal Equation**: No requiere scaling
- **Gradient Descent**: Requiere scaling (preferir standardization)

### RegularizaciÃ³n

Para evitar overfitting, penaliza parÃ¡metros grandes:

#### 1. **Ridge Regression (L2)**

```
J(Î¸) = MSE + Î» Ã— Î£Î¸jÂ²
```

**CaracterÃ­sticas:**
- Reduce magnitud de todos los Î¸
- No elimina features (Î¸ â‰ˆ 0, pero no = 0)
- Mejor cuando todas las features son relevantes

#### 2. **Lasso Regression (L1)**

```
J(Î¸) = MSE + Î» Ã— Î£|Î¸j|
```

**CaracterÃ­sticas:**
- Puede hacer Î¸ = 0 exacto
- Hace feature selection
- Mejor cuando pocas features son relevantes

#### 3. **Elastic Net (L1 + L2)**

```
J(Î¸) = MSE + Î»1 Ã— Î£|Î¸j| + Î»2 Ã— Î£Î¸jÂ²
```

**CaracterÃ­sticas:**
- Combina ventajas de L1 y L2
- MÃ¡s robusto que Lasso
- Requiere ajustar 2 hiperparÃ¡metros

---

## ğŸ¯ Escenario

Tienes datos de casas y quieres predecir precios:

```
Datos de entrenamiento:
TamaÃ±o (mÂ²) | Habitaciones | Precio ($k)
50          | 1            | 100
80          | 2            | 160
100         | 2            | 200
120         | 3            | 240
150         | 3            | 300
```

**Objetivo:** Crear un modelo que prediga el precio basado en tamaÃ±o y habitaciones.

```
Modelo: precio = Î¸0 + Î¸1Ã—tamaÃ±o + Î¸2Ã—habitaciones
```

**PredicciÃ³n:**
```
Casa nueva: 90mÂ², 2 habitaciones
Â¿Precio predicho?
```

---

## ğŸ“ Instrucciones

### Parte 1: EcuaciÃ³n Normal

Implementa la soluciÃ³n analÃ­tica:

```typescript
export interface LinearRegressionModel {
  theta: number[];
  intercept: number;
}

export function fitNormalEquation(
  X: number[][],
  y: number[]
): LinearRegressionModel {
  // Tu cÃ³digo aquÃ­
  // 1. AÃ±adir columna de bias (1s) a X
  // 2. Calcular Î¸ = (X^T Ã— X)^-1 Ã— X^T Ã— y
  // 3. Retornar modelo con theta
}

// Ejemplo de uso:
const X = [[50, 1], [80, 2], [100, 2], [120, 3], [150, 3]];
const y = [100, 160, 200, 240, 300];
const model = fitNormalEquation(X, y);
```

### Parte 2: Gradient Descent

Implementa la soluciÃ³n iterativa:

```typescript
export interface GDConfig {
  learningRate: number;
  iterations: number;
  tolerance?: number;
}

export interface TrainingResult {
  model: LinearRegressionModel;
  costs: number[];
  iterations: number;
}

export function fitGradientDescent(
  X: number[][],
  y: number[],
  config: GDConfig
): TrainingResult {
  // Tu cÃ³digo aquÃ­
  // 1. Normalizar features (importante!)
  // 2. Usar gradient descent del ejercicio 05
  // 3. Retornar modelo y mÃ©tricas
}
```

### Parte 3: Predicciones

Implementa funciones de predicciÃ³n:

```typescript
export function predict(
  X: number[][],
  model: LinearRegressionModel
): number[] {
  // Tu cÃ³digo aquÃ­
  // Å· = X Ã— Î¸
}

export function predictOne(
  x: number[],
  model: LinearRegressionModel
): number {
  // PredicciÃ³n para un solo ejemplo
}
```

### Parte 4: MÃ©tricas

Implementa funciones de evaluaciÃ³n:

```typescript
export function computeMSE(y_true: number[], y_pred: number[]): number {
  // MSE = (1/m) Ã— Î£(Å·i - yi)Â²
}

export function computeRMSE(y_true: number[], y_pred: number[]): number {
  // RMSE = âˆšMSE
}

export function computeMAE(y_true: number[], y_pred: number[]): number {
  // MAE = (1/m) Ã— Î£|Å·i - yi|
}

export function computeR2(y_true: number[], y_pred: number[]): number {
  // RÂ² = 1 - (SS_res / SS_tot)
}
```

### Parte 5: Feature Scaling

Implementa normalizaciÃ³n:

```typescript
export interface ScalerParams {
  mean: number[];
  std: number[];
  min?: number[];
  max?: number[];
}

export function standardize(X: number[][]): {
  X_scaled: number[][];
  params: ScalerParams;
} {
  // Standardization: (x - Î¼) / Ïƒ
}

export function normalize(X: number[][]): {
  X_scaled: number[][];
  params: ScalerParams;
} {
  // Normalization: (x - min) / (max - min)
}

export function applyScaling(
  X: number[][],
  params: ScalerParams
): number[][] {
  // Aplicar scaling con parÃ¡metros existentes
}
```

### Parte 6: RegularizaciÃ³n (Bonus)

Implementa Ridge Regression:

```typescript
export function fitRidge(
  X: number[][],
  y: number[],
  lambda: number
): LinearRegressionModel {
  // Tu cÃ³digo aquÃ­
  // Î¸ = (X^TÃ—X + Î»Ã—I)^-1 Ã— X^T Ã— y
}
```

### Parte 7: ValidaciÃ³n

Implementa train/test split:

```typescript
export interface TrainTestSplit {
  X_train: number[][];
  X_test: number[][];
  y_train: number[];
  y_test: number[];
}

export function trainTestSplit(
  X: number[][],
  y: number[],
  testSize: number = 0.2,
  shuffle: boolean = true
): TrainTestSplit {
  // Dividir datos en train y test
}
```

---

## âœ… Resultado Esperado

Al finalizar, deberÃ­as poder:

1. âœ… Implementar regresiÃ³n lineal con ecuaciÃ³n normal
2. âœ… Implementar regresiÃ³n lineal con gradient descent
3. âœ… Hacer predicciones precisas
4. âœ… Calcular MSE, RMSE, MAE, RÂ²
5. âœ… Normalizar y estandarizar features
6. âœ… Comparar ambos mÃ©todos de entrenamiento
7. âœ… Entender cuÃ¡ndo usar cada mÃ©todo

---

## ğŸ§ª Tests

Ejecuta los tests para verificar tu implementaciÃ³n:

```bash
npm test 06-regresion-lineal
```

Los tests verificarÃ¡n:
- EcuaciÃ³n normal da soluciÃ³n correcta
- Gradient descent converge a la misma soluciÃ³n
- Predicciones son precisas
- MÃ©tricas se calculan correctamente
- Feature scaling funciona
- Manejo de edge cases

---

## ğŸ’¡ Consejos

1. **Normaliza siempre con GD**: Gradient descent requiere features normalizadas
2. **Normal equation para pocas features**: Es exacto y rÃ¡pido
3. **GD para muchas features**: Escala mejor
4. **Verifica dimensiones**: X debe ser (m Ã— n), y debe ser (m,)
5. **Divide train/test**: Siempre evalÃºa en datos no vistos
6. **Visualiza residuos**: DeberÃ­an ser aleatorios, no tener patrÃ³n
7. **RÂ² > 0.7 es bueno**: Depende del problema

---

## ğŸ“ Conceptos Clave

- **Linearidad**: Asume relaciÃ³n lineal entre X e y
- **Independencia**: Los errores deben ser independientes
- **Homoscedasticidad**: Varianza de errores constante
- **Normalidad de residuos**: Para inferencia estadÃ­stica
- **No multicolinealidad**: Features no deben ser altamente correlacionadas
- **Bias-Variance Tradeoff**: RegularizaciÃ³n controla complejidad

---

## ğŸ“Š ComparaciÃ³n de MÃ©todos

| MÃ©todo | Tiempo | PrecisiÃ³n | Escalabilidad | CuÃ¡ndo usar |
|--------|--------|-----------|---------------|-------------|
| Normal Equation | O(nÂ³) | Exacta | Baja (n < 10k) | Pocas features |
| Batch GD | O(kÃ—mÃ—n) | Muy buena | Alta | Muchas features |
| SGD | O(kÃ—n) | Buena | Muy alta | Datasets enormes |

---

## ğŸ“š Recursos

- [Linear Regression - Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [An Introduction to Statistical Learning](https://www.statlearning.com/)
- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)
- [Linear Regression Assumptions](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)

---

**Â¡Comienza implementando en `linear-regression.ts`!** ğŸš€
