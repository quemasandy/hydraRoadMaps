# Ejercicio 08: Clustering K-Means

**Objetivo:** Implementar el algoritmo K-Means desde cero para agrupar datos sin supervisiÃ³n, incluyendo el mÃ©todo del codo para seleccionar K Ã³ptimo.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© es Clustering?

Clustering es **aprendizaje no supervisado**: agrupar datos similares sin labels.

**Usos:**
- **SegmentaciÃ³n de clientes**: Agrupar clientes por comportamiento
- **CompresiÃ³n de imÃ¡genes**: Reducir colores
- **DetecciÃ³n de anomalÃ­as**: Puntos que no pertenecen a ningÃºn cluster
- **AnÃ¡lisis exploratorio**: Descubrir estructura en los datos

### K-Means Algorithm

El algoritmo mÃ¡s popular de clustering:

**Objetivo:** Dividir n puntos en K clusters minimizando la varianza intra-cluster.

**FunciÃ³n de costo (Inertia):**
```
J = Î£ Î£ ||xi - Î¼k||Â²
    kâˆˆK iâˆˆCk
```

Donde:
- `Î¼k`: Centroide del cluster k
- `Ck`: Conjunto de puntos en cluster k
- `||.||`: Distancia euclidiana

**Algoritmo:**

```
1. Inicializar K centroides (aleatoriamente o K-Means++)
2. Repetir hasta convergencia:
   a. AsignaciÃ³n: Asignar cada punto al centroide mÃ¡s cercano
   b. ActualizaciÃ³n: Recalcular centroides como media de puntos
3. Retornar centroides y asignaciones
```

**Convergencia:** Cuando las asignaciones no cambian o inertia no mejora.

### InicializaciÃ³n: K-Means++

La inicializaciÃ³n aleatoria puede dar malos resultados. **K-Means++** mejora esto:

**Algoritmo:**
```
1. Elegir primer centroide aleatoriamente
2. Para cada nuevo centroide:
   a. Calcular distancia D(x) de cada punto al centroide mÃ¡s cercano
   b. Elegir nuevo centroide con probabilidad âˆ D(x)Â²
3. Repetir hasta tener K centroides
```

**Ventaja:** Centroides iniciales estÃ¡n bien dispersos â†’ mejor convergencia.

### MÃ©todo del Codo (Elbow Method)

**Problema:** Â¿CÃ³mo elegir K?

**SoluciÃ³n:** Graficar inertia vs K:

```
Inertia
  |
  |\
  | \
  |  \___
  |      ----____
  |____________
  1  2  3  4  5  K
       â†‘
     "Codo"
```

**InterpretaciÃ³n:**
- K pequeÃ±o: Alta inertia (clusters grandes)
- K grande: Baja inertia (muchos clusters)
- **"Codo"**: Punto donde inertia deja de disminuir significativamente

### Limitaciones de K-Means

1. **Requiere especificar K**: No siempre es obvio
2. **Sensible a inicializaciÃ³n**: Puede converger a mÃ­nimos locales
3. **Asume clusters esfÃ©ricos**: No funciona bien con formas irregulares
4. **Sensible a escala**: Features con mayor rango dominan
5. **Sensible a outliers**: Pueden distorsionar centroides

**Soluciones:**
- K-Means++: Mejor inicializaciÃ³n
- Ejecutar mÃºltiples veces: Elegir mejor resultado
- Normalizar features: Usar StandardScaler
- Remover outliers: Pre-procesamiento
- Alternativas: DBSCAN, GMM para clusters no esfÃ©ricos

### MÃ©tricas de EvaluaciÃ³n

#### 1. **Inertia (Within-Cluster Sum of Squares)**

```
Inertia = Î£ Î£ ||xi - Î¼k||Â²
```

**CaracterÃ­sticas:**
- Siempre disminuye al aumentar K
- No normalizada
- Ãštil para mÃ©todo del codo

#### 2. **Silhouette Score**

Para cada punto:
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))

a(i): Distancia promedio a puntos en su cluster
b(i): Distancia promedio al cluster mÃ¡s cercano
```

**InterpretaciÃ³n:**
- `s(i) â‰ˆ 1`: Bien asignado
- `s(i) â‰ˆ 0`: En el lÃ­mite entre clusters
- `s(i) < 0`: Probablemente mal asignado

**Silhouette Score promedio:**
```
Silhouette = (1/n) Ã— Î£ s(i)
```

Rango: [-1, 1], mayor es mejor.

---

## ğŸ¯ Escenario

Tienes datos de clientes (ingresos, edad) y quieres segmentarlos:

```
Datos:
Ingresos ($k) | Edad | Cluster
30            | 25   | ?
35            | 28   | ?
80            | 45   | ?
75            | 40   | ?
150           | 55   | ?
```

**Objetivo:** Agrupar clientes en K segmentos para marketing dirigido.

---

## ğŸ“ Instrucciones

### Parte 1: K-Means BÃ¡sico

```typescript
export interface KMeansResult {
  centroids: number[][];
  labels: number[];
  inertia: number;
  iterations: number;
}

export function kMeans(
  X: number[][],
  K: number,
  maxIterations: number = 100,
  tolerance: number = 1e-4
): KMeansResult {
  // 1. Inicializar centroides aleatoriamente
  // 2. Repetir:
  //    - Asignar puntos al centroide mÃ¡s cercano
  //    - Recalcular centroides
  //    - Verificar convergencia
}
```

### Parte 2: K-Means++

```typescript
export function kMeansPlusPlus(
  X: number[][],
  K: number,
  maxIterations: number = 100
): KMeansResult {
  // 1. Inicializar con K-Means++
  // 2. Ejecutar K-Means normal
}
```

### Parte 3: MÃ©todo del Codo

```typescript
export interface ElbowResult {
  K_values: number[];
  inertias: number[];
  optimalK: number;
}

export function elbowMethod(
  X: number[][],
  maxK: number = 10
): ElbowResult {
  // Ejecutar K-Means para K=1...maxK
  // Encontrar "codo" automÃ¡ticamente
}
```

### Parte 4: Utilidades

```typescript
export function euclideanDistance(a: number[], b: number[]): number;
export function computeInertia(
  X: number[][],
  centroids: number[][],
  labels: number[]
): number;
export function silhouetteScore(
  X: number[][],
  labels: number[]
): number;
```

---

## âœ… Resultado Esperado

1. âœ… Implementar K-Means desde cero
2. âœ… Implementar K-Means++
3. âœ… Calcular inertia
4. âœ… MÃ©todo del codo automÃ¡tico
5. âœ… Silhouette score
6. âœ… Ejecutar mÃºltiples veces y elegir mejor

---

## ğŸ§ª Tests

```bash
npm test 08-clustering-kmeans
```

---

## ğŸ’¡ Consejos

1. **Normaliza datos**: StandardScaler antes de K-Means
2. **Ejecuta mÃºltiples veces**: Elige resultado con menor inertia
3. **K tÃ­pico**: 2-10 para la mayorÃ­a de problemas
4. **Convergencia**: 10-50 iteraciones suelen bastar
5. **Silhouette > 0.5**: Buen clustering
6. **Visualiza**: Grafica puntos y centroides

---

## ğŸ“š Recursos

- [K-Means - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)
- [K-Means++ Paper](http://ilpubs.stanford.edu:8090/778/)
- [Silhouette Analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

---

**Â¡Comienza implementando en `kmeans.ts`!** ğŸš€
