# Ejercicio 10: Ensemble Methods

**Objetivo:** Implementar m√©todos de ensemble (bagging y random forest) para mejorar el rendimiento combinando m√∫ltiples modelos.

## üìñ Teor√≠a

### ¬øQu√© son los Ensemble Methods?

**Idea central:** "La sabidur√≠a de las multitudes"

Combinar m√∫ltiples modelos d√©biles para crear un modelo fuerte.

**Ventajas:**
- **Mejor accuracy**: Supera modelos individuales
- **Reduce overfitting**: Promedia errores
- **M√°s robusto**: Menos sensible a ruido
- **Reduce varianza**: Predicciones m√°s estables

### Bagging (Bootstrap Aggregating)

**Algoritmo:**
```
1. Para i = 1 a M:
   a. Tomar muestra bootstrap de datos (con reemplazo)
   b. Entrenar modelo en esta muestra
2. Predicci√≥n:
   - Clasificaci√≥n: Votaci√≥n mayoritaria
   - Regresi√≥n: Promedio
```

**Bootstrap Sample:**
```
Datos originales: [1, 2, 3, 4, 5]
Bootstrap 1:      [1, 1, 3, 4, 5]
Bootstrap 2:      [2, 2, 2, 3, 5]
Bootstrap 3:      [1, 3, 4, 4, 5]
```

Cada muestra tiene mismo tama√±o, pero con reemplazo.

**¬øPor qu√© funciona?**
- Reduce varianza sin aumentar bias
- Modelos ven diferentes datos
- Errores se cancelan al promediar

### Random Forest

**Mejora sobre bagging:** A√±ade aleatoriedad en features.

**Algoritmo:**
```
1. Para cada √°rbol:
   a. Tomar muestra bootstrap de datos
   b. En cada split:
      - Elegir subconjunto aleatorio de m features
      - Encontrar mejor split entre esas m features
2. Predicci√≥n: Votaci√≥n/promedio de todos los √°rboles
```

**Par√°metro clave:**
```
m = sqrt(n_features) para clasificaci√≥n
m = n_features / 3 para regresi√≥n
```

**Ventajas adicionales:**
- Reduce correlaci√≥n entre √°rboles
- Mejor generalizaci√≥n
- Maneja miles de features
- Feature importance autom√°tico

### Boosting (Conceptos b√°sicos)

**Idea:** Entrenar modelos secuencialmente, enfoc√°ndose en errores previos.

**AdaBoost (Adaptive Boosting):**
```
1. Inicializar pesos wi = 1/n para cada ejemplo
2. Para t = 1 a T:
   a. Entrenar clasificador ht con pesos wi
   b. Calcular error Œµt = Œ£(wi √ó I(yi ‚â† ht(xi)))
   c. Calcular Œ±t = log((1-Œµt)/Œµt)
   d. Actualizar pesos: wi = wi √ó exp(Œ±t √ó I(yi ‚â† ht(xi)))
3. Predicci√≥n: sign(Œ£(Œ±t √ó ht(x)))
```

**Diferencia con Bagging:**
- Bagging: Modelos paralelos e independientes
- Boosting: Modelos secuenciales, cada uno corrige errores del anterior

### Comparaci√≥n de M√©todos

| M√©todo | Entrenamiento | Varianza | Bias | Overfitting |
|--------|---------------|----------|------|-------------|
| Single Tree | R√°pido | Alta | Baja | Alto |
| Bagging | Paralelo | Baja | Igual | Medio |
| Random Forest | Paralelo | Muy baja | Igual | Bajo |
| Boosting | Secuencial | Baja | Baja | Medio-Alto |

**Recomendaci√≥n general:** Random Forest es excelente punto de partida.

---

## üéØ Escenario

Dataset de pr√©stamos bancarios:

```
Edad | Ingresos | Deuda | Default
25   | 40k      | 10k   | No
30   | 60k      | 5k    | No
35   | 50k      | 30k   | S√≠
40   | 90k      | 20k   | No
```

**Objetivo:** Predecir si un cliente har√° default en su pr√©stamo.

---

## üìù Instrucciones

### Parte 1: Bagging Classifier

```typescript
export class BaggingClassifier {
  private models: DecisionTreeClassifier[] = [];
  private nEstimators: number;
  private maxSamples: number;

  constructor(nEstimators: number = 10, maxSamples: number = 1.0) {
    this.nEstimators = nEstimators;
    this.maxSamples = maxSamples;
  }

  fit(X: number[][], y: number[]): void {
    // 1. Para cada estimador:
    //    - Crear muestra bootstrap
    //    - Entrenar √°rbol de decisi√≥n
  }

  predict(X: number[][]): number[] {
    // Votaci√≥n mayoritaria de todos los √°rboles
  }
}
```

### Parte 2: Random Forest

```typescript
export class RandomForestClassifier {
  private trees: DecisionTreeClassifier[] = [];
  private nEstimators: number;
  private maxFeatures: number | 'sqrt' | 'log2';

  constructor(config: {
    nEstimators?: number;
    maxFeatures?: number | 'sqrt' | 'log2';
    maxDepth?: number;
  }) {
    // Configuraci√≥n
  }

  fit(X: number[][], y: number[]): void {
    // Entrenar m√∫ltiples √°rboles con feature randomness
  }

  predict(X: number[][]): number[];
  predictProba(X: number[][]): number[][];
  getFeatureImportance(): number[];
}
```

### Parte 3: Utilidades

```typescript
export function bootstrapSample(
  X: number[][],
  y: number[]
): { X_sample: number[][]; y_sample: number[] };

export function randomSubset<T>(arr: T[], size: number): T[];

export function majorityVote(predictions: number[][]): number[];

export function outOfBagScore(
  model: BaggingClassifier | RandomForestClassifier,
  X: number[][],
  y: number[]
): number;
```

---

## ‚úÖ Resultado Esperado

1. ‚úÖ Implementar Bagging desde cero
2. ‚úÖ Implementar Random Forest
3. ‚úÖ Bootstrap sampling
4. ‚úÖ Votaci√≥n mayoritaria
5. ‚úÖ Feature importance
6. ‚úÖ Out-of-bag scoring

---

## üß™ Tests

```bash
npm test 10-ensemble-methods
```

---

## üí° Consejos

1. **n_estimators=100**: Buen punto de partida
2. **max_features='sqrt'**: Para clasificaci√≥n
3. **M√°s √°rboles = mejor**: Pero m√°s lento
4. **max_depth=10**: Previene overfitting individual
5. **OOB score**: Estima accuracy sin validation set
6. **Feature importance**: Identifica variables clave

---

## üìö Recursos

- [Random Forest - sklearn](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [Bagging Predictors - Breiman](https://www.stat.berkeley.edu/~breiman/bagging.pdf)
- [Random Forest Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)

---

**¬°Comienza implementando en `ensemble.ts`!** üöÄ
