# Ejercicio 12: Backpropagation

**Objetivo:** Implementar el algoritmo de backpropagation desde cero para entrenar redes neuronales, entendiendo la regla de la cadena y la actualizaciÃ³n de pesos.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© es Backpropagation?

**Algoritmo fundamental** para entrenar redes neuronales (1986, Rumelhart, Hinton, Williams).

**Idea central:** Propagar el error desde la salida hacia atrÃ¡s, calculando gradientes para actualizar pesos.

```
Forward Pass:  Input â†’ Hidden â†’ Output â†’ Loss
                 â†“        â†“        â†“       â†“
Backward Pass:  âˆ‡Wâ‚  â†  âˆ‡Wâ‚‚   â†  âˆ‡Å·   â†  âˆ‡L
```

**Â¿Por quÃ© es importante?**
- **Eficiencia:** Calcula todos los gradientes en una pasada
- **Preciso:** Usa regla de la cadena matemÃ¡tica exacta
- **Escalable:** Funciona para redes de cualquier profundidad

### La Regla de la Cadena

**Fundamento matemÃ¡tico** de backpropagation.

**En cÃ¡lculo:**
```
Si y = f(g(x)), entonces:
dy/dx = (dy/dg) Ã— (dg/dx)
```

**En redes neuronales:**
```
Si Loss = L(Å·) y Å· = Ïƒ(z) y z = WÃ—x + b:

âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚Å·) Ã— (âˆ‚Å·/âˆ‚z) Ã— (âˆ‚z/âˆ‚W)
        â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜
        error     derivada   input
                  activaciÃ³n
```

**Ejemplo numÃ©rico:**
```
Forward:
z = 2Ã—3 + 1 = 7
Å· = Ïƒ(7) = 0.999
L = (1 - 0.999)Â² = 0.000001

Backward:
âˆ‚L/âˆ‚Å· = 2(Å· - y) = 2(0.999 - 1) = -0.002
âˆ‚Å·/âˆ‚z = Ïƒ'(7) = Ïƒ(7)Ã—(1-Ïƒ(7)) = 0.001
âˆ‚z/âˆ‚W = x = 3

âˆ‚L/âˆ‚W = (-0.002) Ã— (0.001) Ã— 3 = -0.000006
```

### Algoritmo Backpropagation (2 capas)

**Arquitectura:** Input â†’ Hidden â†’ Output

**Forward Pass:**
```
1. zâ‚ = Wâ‚Ã—x + bâ‚          (suma ponderada hidden)
2. h = Ïƒ(zâ‚)               (activaciÃ³n hidden)
3. zâ‚‚ = Wâ‚‚Ã—h + bâ‚‚          (suma ponderada output)
4. Å· = Ïƒ(zâ‚‚)               (activaciÃ³n output)
5. L = (y - Å·)Â²            (loss)
```

**Backward Pass:**
```
1. Gradiente output layer:
   âˆ‚L/âˆ‚Å· = 2(Å· - y)                    â† error en output
   âˆ‚Å·/âˆ‚zâ‚‚ = Ïƒ'(zâ‚‚)                     â† derivada activaciÃ³n
   Î´â‚‚ = âˆ‚L/âˆ‚Å· Ã— âˆ‚Å·/âˆ‚zâ‚‚                â† error term output

2. Gradientes Wâ‚‚ y bâ‚‚:
   âˆ‚L/âˆ‚Wâ‚‚ = Î´â‚‚ Ã— háµ€                    â† gradient weight
   âˆ‚L/âˆ‚bâ‚‚ = Î´â‚‚                         â† gradient bias

3. Gradiente hidden layer (chain rule):
   âˆ‚L/âˆ‚h = Wâ‚‚áµ€ Ã— Î´â‚‚                    â† error propagado
   âˆ‚h/âˆ‚zâ‚ = Ïƒ'(zâ‚)                     â† derivada activaciÃ³n
   Î´â‚ = âˆ‚L/âˆ‚h âŠ™ âˆ‚h/âˆ‚zâ‚                 â† error term hidden (âŠ™ = element-wise)

4. Gradientes Wâ‚ y bâ‚:
   âˆ‚L/âˆ‚Wâ‚ = Î´â‚ Ã— xáµ€                    â† gradient weight
   âˆ‚L/âˆ‚bâ‚ = Î´â‚                         â† gradient bias
```

**Update (Gradient Descent):**
```
Wâ‚ = Wâ‚ - Î· Ã— âˆ‚L/âˆ‚Wâ‚
bâ‚ = bâ‚ - Î· Ã— âˆ‚L/âˆ‚bâ‚
Wâ‚‚ = Wâ‚‚ - Î· Ã— âˆ‚L/âˆ‚Wâ‚‚
bâ‚‚ = bâ‚‚ - Î· Ã— âˆ‚L/âˆ‚bâ‚‚
```

### Derivadas de Funciones de ActivaciÃ³n

**Sigmoid:**
```
Ïƒ(z) = 1/(1 + eâ»á¶»)
Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))
```

**Tanh:**
```
tanh(z) = (eá¶» - eâ»á¶»)/(eá¶» + eâ»á¶»)
tanh'(z) = 1 - tanhÂ²(z)
```

**ReLU:**
```
ReLU(z) = max(0, z)
ReLU'(z) = { 1 if z > 0
           { 0 if z â‰¤ 0
```

### Funciones de Loss

**Mean Squared Error (MSE) - RegresiÃ³n:**
```
L = (1/m) Ã— Î£(yáµ¢ - Å·áµ¢)Â²
âˆ‚L/âˆ‚Å· = (2/m) Ã— (Å· - y)
```

**Binary Cross-Entropy - ClasificaciÃ³n:**
```
L = -(1/m) Ã— Î£[yÃ—log(Å·) + (1-y)Ã—log(1-Å·)]
âˆ‚L/âˆ‚Å· = (Å· - y) / [Å·Ã—(1-Å·)]
```

### Vanishing Gradient Problem

**Problema:** Gradientes muy pequeÃ±os en capas iniciales.

**Causa:**
```
Î´â‚ = Î´â‚‚ Ã— Wâ‚‚áµ€ Ã— Ïƒ'(zâ‚)
     â””â”€propagadoâ”€â”˜ â””â”€derivadaâ”€â”˜
```

Si Ïƒ'(zâ‚) < 1 (como en sigmoid: max=0.25), el gradiente se "desvanece" multiplicÃ¡ndose por valores < 1 en cada capa.

**Ejemplo numÃ©rico (red de 3 capas con sigmoid):**
```
Capa 3: Î´â‚ƒ = 1.0
Capa 2: Î´â‚‚ = Î´â‚ƒ Ã— Wâ‚ƒáµ€ Ã— Ïƒ'(zâ‚‚) = 1.0 Ã— 0.5 Ã— 0.25 = 0.125
Capa 1: Î´â‚ = Î´â‚‚ Ã— Wâ‚‚áµ€ Ã— Ïƒ'(zâ‚) = 0.125 Ã— 0.5 Ã— 0.25 = 0.0156
```

Capa 1 recibe gradiente ~64x mÃ¡s pequeÃ±o que capa 3!

**Soluciones:**
1. **ReLU:** Derivada = 1 para z > 0 (no satura)
2. **Batch Normalization:** Normaliza activaciones
3. **Residual Connections:** Skip connections (ResNet)
4. **Better initialization:** Xavier/He
5. **Lower learning rate** en capas profundas

### Gradient Checking

**Verificar** que backpropagation estÃ¡ correcto.

**MÃ©todo:** AproximaciÃ³n numÃ©rica del gradiente.

```
Gradiente analÃ­tico: âˆ‚L/âˆ‚w (calculado por backprop)

Gradiente numÃ©rico:
âˆ‚L/âˆ‚w â‰ˆ [L(w + Îµ) - L(w - Îµ)] / (2Îµ)
donde Îµ = 1e-7
```

**ComparaciÃ³n:**
```
difference = |grad_analytic - grad_numeric| /
             max(|grad_analytic|, |grad_numeric|)

âœ“ difference < 1e-7: Excelente
âœ“ difference < 1e-5: Bueno
âœ— difference > 1e-3: Error en implementaciÃ³n
```

---

## ğŸ¯ Escenario

**Dataset:** ClasificaciÃ³n binaria de flores Iris

```
Sepal Length | Sepal Width | Clase
5.1          | 3.5         | 0 (Setosa)
6.2          | 2.9         | 1 (No Setosa)
```

**Objetivo:** Entrenar red neuronal con backpropagation para clasificar correctamente.

---

## ğŸ“ Instrucciones

### Parte 1: Funciones de Loss

```typescript
export function meanSquaredError(
  y_true: number[],
  y_pred: number[]
): number;

export function meanSquaredErrorDerivative(
  y_true: number[],
  y_pred: number[]
): number[];

export function binaryCrossEntropy(
  y_true: number[],
  y_pred: number[]
): number;

export function binaryCrossEntropyDerivative(
  y_true: number[],
  y_pred: number[]
): number[];
```

### Parte 2: MLP con Backpropagation

```typescript
export interface BackpropConfig {
  inputSize: number;
  hiddenSize: number;
  outputSize: number;
  activation?: 'sigmoid' | 'tanh' | 'relu';
  learningRate?: number;
  lossFunction?: 'mse' | 'binary_crossentropy';
}

export interface TrainingHistory {
  epochs: number[];
  losses: number[];
  accuracies?: number[];
}

export class MLPWithBackprop {
  constructor(config: BackpropConfig) {
    // Inicializar red
  }

  forward(X: number[][]): {
    z1: number[][];
    h: number[][];
    z2: number[][];
    output: number[][];
  } {
    // Forward pass guardando valores intermedios
    // Necesarios para backward pass
  }

  backward(
    X: number[][],
    y: number[][],
    cache: {
      z1: number[][];
      h: number[][];
      z2: number[][];
      output: number[][];
    }
  ): void {
    // Backward pass
    // 1. Calcular Î´â‚‚ (error output layer)
    // 2. Calcular gradientes âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚
    // 3. Propagar error: calcular Î´â‚
    // 4. Calcular gradientes âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚
    // 5. Actualizar pesos con gradient descent
  }

  fit(
    X: number[][],
    y: number[][],
    epochs: number,
    verbose?: boolean
  ): TrainingHistory {
    // Entrenar red con backpropagation
  }

  predict(X: number[][]): number[];
  predictProba(X: number[][]): number[][];
}
```

### Parte 3: Gradient Checking

```typescript
export function numericalGradient(
  computeLoss: (w: number) => number,
  w: number,
  epsilon?: number
): number;

export function checkGradients(
  model: MLPWithBackprop,
  X: number[][],
  y: number[][],
  epsilon?: number
): {
  maxDifference: number;
  avgDifference: number;
  isCorrect: boolean;
};
```

### Parte 4: Utilidades

```typescript
export function oneHotEncode(y: number[], numClasses: number): number[][];

export function computeAccuracy(
  y_true: number[],
  y_pred: number[]
): number;

export function shuffle(
  X: number[][],
  y: number[][]
): { X_shuffled: number[][]; y_shuffled: number[][] };
```

---

## âœ… Resultado Esperado

1. âœ… Implementar MSE y Binary Cross-Entropy con derivadas
2. âœ… MLP con forward pass completo (guardando valores intermedios)
3. âœ… Backward pass con chain rule
4. âœ… ActualizaciÃ³n de pesos con gradient descent
5. âœ… Entrenar red y ver loss disminuir
6. âœ… Gradient checking para verificar implementaciÃ³n
7. âœ… Visualizar curva de aprendizaje

---

## ğŸ§ª Tests

```bash
npm test 12-backpropagation
```

---

## ğŸ’¡ Consejos

1. **Guardar valores intermedios** en forward pass (z1, h, z2, Å·)
2. **Implementar paso a paso:** Primero forward, luego backward
3. **Verificar con gradient checking** antes de entrenar
4. **Learning rate:** Empezar con 0.01, ajustar si diverge
5. **Batch vs Sample:** Esta implementaciÃ³n usa batch completo
6. **Debugging:** Imprimir shapes de matrices para verificar dimensiones
7. **Vanishing gradient:** Si no aprende, probar ReLU o aumentar learning rate

---

## ğŸ“Š MatemÃ¡ticas Detalladas

**Gradiente de Wâ‚‚ (output layer):**
```
Para un ejemplo (x, y):

Forward:
h = Ïƒ(Wâ‚Ã—x + bâ‚)
Å· = Ïƒ(Wâ‚‚Ã—h + bâ‚‚)
L = (y - Å·)Â²

Backward:
âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚Å· Ã— âˆ‚Å·/âˆ‚zâ‚‚ Ã— âˆ‚zâ‚‚/âˆ‚Wâ‚‚

donde:
âˆ‚L/âˆ‚Å· = -2(y - Å·)           â† derivada del loss
âˆ‚Å·/âˆ‚zâ‚‚ = Ïƒ'(zâ‚‚)             â† derivada sigmoid
âˆ‚zâ‚‚/âˆ‚Wâ‚‚ = háµ€                â† derivada de z = WÃ—h + b

Combinando:
Î´â‚‚ = -2(y - Å·) âŠ™ Ïƒ'(zâ‚‚)     â† error term
âˆ‚L/âˆ‚Wâ‚‚ = Î´â‚‚ Ã— háµ€            â† gradient (outer product)
```

**Gradiente de Wâ‚ (hidden layer):**
```
Propagamos error desde output:

âˆ‚L/âˆ‚h = Wâ‚‚áµ€ Ã— Î´â‚‚             â† error propagado desde output
âˆ‚h/âˆ‚zâ‚ = Ïƒ'(zâ‚)              â† derivada activaciÃ³n hidden
Î´â‚ = âˆ‚L/âˆ‚h âŠ™ Ïƒ'(zâ‚)          â† error term hidden
âˆ‚L/âˆ‚Wâ‚ = Î´â‚ Ã— xáµ€             â† gradient
```

**Dimensiones (crucial para implementaciÃ³n):**
```
Batch de m ejemplos:

X:  (m, n_in)
Wâ‚: (n_h, n_in)     Wâ‚Ã—Xáµ€: (n_h, m)
bâ‚: (n_h, 1)        â†’  (n_h, m) [broadcasting]
h:  (m, n_h)

Wâ‚‚: (n_out, n_h)    Wâ‚‚Ã—háµ€: (n_out, m)
bâ‚‚: (n_out, 1)      â†’  (n_out, m) [broadcasting]
Å·:  (m, n_out)

Gradientes:
âˆ‚L/âˆ‚Wâ‚‚: (n_out, n_h)  â† mismo shape que Wâ‚‚
âˆ‚L/âˆ‚Wâ‚: (n_h, n_in)   â† mismo shape que Wâ‚
```

---

## ğŸ“š Recursos

- [Backpropagation - 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)
- [Backprop Calculus - 3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
- [CS231n: Backpropagation](http://cs231n.github.io/optimization-2/)
- [Gradient Checking](https://www.coursera.org/learn/deep-neural-network/lecture/Y3s6r/gradient-checking)

---

**Â¡Comienza implementando las funciones de loss en `backpropagation.ts`!** ğŸ”™
