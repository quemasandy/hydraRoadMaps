# Ejercicio 07: Clasificaci√≥n Log√≠stica

**Objetivo:** Implementar regresi√≥n log√≠stica desde cero para clasificaci√≥n binaria y multiclase, usando la funci√≥n sigmoid y cross-entropy loss.

## üìñ Teor√≠a

### ¬øQu√© es la Regresi√≥n Log√≠stica?

A pesar del nombre "regresi√≥n", se usa para **clasificaci√≥n**. Es fundamental en ML:
- **Clasificaci√≥n binaria**: Spam/No spam, Fraude/Leg√≠timo
- **Clasificaci√≥n multiclase**: D√≠gitos (0-9), Categor√≠as
- **Probabilidades**: Devuelve probabilidades interpretables
- **Baseline**: Punto de partida antes de modelos complejos

### Clasificaci√≥n Binaria

#### El Modelo

Para clasificaci√≥n binaria (0 o 1):

```
z = Œ∏0 + Œ∏1√óx1 + Œ∏2√óx2 + ... + Œ∏n√óxn = Œ∏^T √ó x
≈∑ = œÉ(z) = 1 / (1 + e^-z)
```

Donde:
- `z`: Combinaci√≥n lineal (logit)
- `œÉ(z)`: Funci√≥n sigmoid (0 a 1)
- `≈∑`: Probabilidad de clase 1

**Decisi√≥n:**
```
Clase 1 si ≈∑ ‚â• 0.5
Clase 0 si ≈∑ < 0.5
```

#### Funci√≥n Sigmoid

```
œÉ(z) = 1 / (1 + e^-z)
```

**Propiedades:**
- Rango: (0, 1) ‚Üí perfecta para probabilidades
- `œÉ(0) = 0.5`: Punto de decisi√≥n
- `œÉ(+‚àû) = 1`: Confianza alta en clase 1
- `œÉ(-‚àû) = 0`: Confianza alta en clase 0
- Derivada: `œÉ'(z) = œÉ(z) √ó (1 - œÉ(z))`

**Forma de S:** Transici√≥n suave entre 0 y 1

```
   1 |           ___---
     |       ___/
 0.5 |    __/
     |___/
   0 |_______________
     -‚àû    0    +‚àû
```

#### Funci√≥n de Costo: Cross-Entropy

Para regresi√≥n lineal us√°bamos MSE, pero para clasificaci√≥n usamos **cross-entropy**:

```
J(Œ∏) = -(1/m) √ó Œ£[yi√ólog(≈∑i) + (1-yi)√ólog(1-≈∑i)]
```

**¬øPor qu√© no MSE?**
- MSE es no-convexa con sigmoid ‚Üí m√∫ltiples m√≠nimos locales
- Cross-entropy es convexa ‚Üí un solo m√≠nimo global
- Penaliza predicciones incorrectas exponencialmente

**Interpretaci√≥n:**
```
Si y = 1: costo = -log(≈∑)
  ≈∑ ‚Üí 1: costo ‚Üí 0 (bueno)
  ≈∑ ‚Üí 0: costo ‚Üí ‚àû (muy malo)

Si y = 0: costo = -log(1-≈∑)
  ≈∑ ‚Üí 0: costo ‚Üí 0 (bueno)
  ≈∑ ‚Üí 1: costo ‚Üí ‚àû (muy malo)
```

#### Gradiente

```
‚àÇJ/‚àÇŒ∏j = (1/m) √ó Œ£(≈∑i - yi) √ó xij
‚àáJ(Œ∏) = (1/m) √ó X^T √ó (≈∑ - y)
```

**¬°Igual que en regresi√≥n lineal!** La diferencia est√° en que `≈∑ = œÉ(X√óŒ∏)`.

### Decision Boundary

El **l√≠mite de decisi√≥n** es donde `œÉ(z) = 0.5`, es decir, `z = 0`:

```
Œ∏0 + Œ∏1√óx1 + Œ∏2√óx2 = 0
```

**Caracter√≠sticas:**
- Es una l√≠nea recta (lineal) en 2D
- Un plano en 3D
- Un hiperplano en n dimensiones

**Limitaci√≥n:** Solo puede separar clases linealmente separables.

**Soluci√≥n:** A√±adir features polinomiales (x1¬≤, x1√óx2, etc.)

### Clasificaci√≥n Multiclase

#### 1. **One-vs-Rest (OvR) / One-vs-All (OvA)**

Entrenar K clasificadores binarios:

```
Clasificador 1: Clase 1 vs (Clase 2, 3, ..., K)
Clasificador 2: Clase 2 vs (Clase 1, 3, ..., K)
...
Clasificador K: Clase K vs (Clase 1, 2, ..., K-1)
```

**Predicci√≥n:**
```
clase = argmax(p1, p2, ..., pK)
```

**Ventajas:**
- F√°cil de implementar
- Funciona con cualquier clasificador binario

**Desventajas:**
- K modelos separados
- Las probabilidades no suman 1

#### 2. **Softmax Regression (Multinomial Logistic)**

Generalizaci√≥n directa de logistic regression:

```
z_k = Œ∏_k^T √ó x  (para cada clase k)
P(y = k | x) = e^z_k / Œ£(e^z_j)
```

**Propiedades:**
- Las probabilidades suman 1
- Un solo modelo para todas las clases
- M√°s eficiente que OvR

**Funci√≥n de costo (Cross-Entropy Multiclase):**
```
J(Œ∏) = -(1/m) √ó Œ£Œ£[1{yi=k} √ó log(P(y=k|xi))]
```

### Regularizaci√≥n

Al igual que en regresi√≥n lineal, podemos regularizar:

#### L2 (Ridge)

```
J(Œ∏) = CrossEntropy + (Œª/2m) √ó Œ£Œ∏j¬≤
```

**Efecto:**
- Reduce overfitting
- Hace el modelo m√°s general
- Estabiliza coeficientes

### M√©tricas de Evaluaci√≥n

#### 1. **Accuracy (Exactitud)**

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Problema:** Enga√±osa con clases desbalanceadas.

Ejemplo: 95% son clase 0
- Modelo que siempre predice 0 ‚Üí 95% accuracy
- ¬°Pero es in√∫til!

#### 2. **Precision (Precisi√≥n)**

```
Precision = TP / (TP + FP)
```

"De los que predije como positivos, ¬øcu√°ntos lo eran?"

#### 3. **Recall (Sensibilidad)**

```
Recall = TP / (TP + FN)
```

"De todos los positivos reales, ¬øcu√°ntos detect√©?"

#### 4. **F1-Score**

```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

Media arm√≥nica de precision y recall.

#### 5. **Confusion Matrix**

```
                Predicho
                0    1
Real    0    [ TN   FP ]
        1    [ FN   TP ]
```

**Interpretaci√≥n:**
- **TN (True Negative)**: Correctamente clasificado como 0
- **TP (True Positive)**: Correctamente clasificado como 1
- **FP (False Positive)**: Error tipo I (falsa alarma)
- **FN (False Negative)**: Error tipo II (fall√≥ en detectar)

---

## üéØ Escenario

Tienes datos de ex√°menes de admisi√≥n y quieres predecir si un estudiante ser√° admitido:

```
Datos de entrenamiento:
Exam1 | Exam2 | Admitido
45    | 85    | 1
52    | 68    | 0
60    | 86    | 1
70    | 96    | 1
72    | 45    | 0
80    | 75    | 1
```

**Objetivo:** Predecir admisi√≥n basado en notas de ex√°menes.

```
Modelo: P(admitido=1) = œÉ(Œ∏0 + Œ∏1√óexam1 + Œ∏2√óexam2)
```

**Predicci√≥n:**
```
Nuevo estudiante: Exam1=65, Exam2=80
¬øProbabilidad de admisi√≥n?
```

---

## üìù Instrucciones

### Parte 1: Funci√≥n Sigmoid

Implementa la funci√≥n sigmoid:

```typescript
export function sigmoid(z: number | number[]): number | number[] {
  // Tu c√≥digo aqu√≠
  // œÉ(z) = 1 / (1 + e^-z)
  // Manejar tanto n√∫meros como arrays
}

// Ejemplos:
sigmoid(0)     // ‚Üí 0.5
sigmoid(10)    // ‚Üí ~1
sigmoid(-10)   // ‚Üí ~0
sigmoid([0, 1, -1]) // ‚Üí [0.5, 0.73, 0.27]
```

### Parte 2: Clasificaci√≥n Binaria

Implementa regresi√≥n log√≠stica binaria:

```typescript
export interface LogisticRegressionModel {
  theta: number[];
  intercept: number;
  scalerParams?: ScalerParams;
}

export interface TrainingConfig {
  learningRate: number;
  iterations: number;
  tolerance?: number;
  normalize?: boolean;
  lambda?: number; // Regularizaci√≥n
}

export interface TrainingResult {
  model: LogisticRegressionModel;
  costs: number[];
  iterations: number;
}

export function fitLogisticRegression(
  X: number[][],
  y: number[],
  config: TrainingConfig
): TrainingResult {
  // Tu c√≥digo aqu√≠
  // 1. Normalizar features si es necesario
  // 2. Inicializar theta
  // 3. Gradient descent:
  //    - Calcular z = X √ó Œ∏
  //    - Calcular ≈∑ = œÉ(z)
  //    - Calcular costo (cross-entropy)
  //    - Calcular gradiente
  //    - Actualizar Œ∏
}
```

### Parte 3: Predicciones

Implementa funciones de predicci√≥n:

```typescript
export function predictProba(
  X: number[][],
  model: LogisticRegressionModel
): number[] {
  // Retorna probabilidades P(y=1)
}

export function predictBinary(
  X: number[][],
  model: LogisticRegressionModel,
  threshold: number = 0.5
): number[] {
  // Retorna clases (0 o 1)
}
```

### Parte 4: Funciones de Costo

Implementa cross-entropy:

```typescript
export function computeCrossEntropy(
  y_true: number[],
  y_pred_proba: number[]
): number {
  // J = -(1/m) √ó Œ£[y√ólog(≈∑) + (1-y)√ólog(1-≈∑)]
}

export function computeCrossEntropyWithRegularization(
  y_true: number[],
  y_pred_proba: number[],
  theta: number[],
  lambda: number
): number {
  // J + (Œª/2m) √ó Œ£Œ∏j¬≤
}
```

### Parte 5: M√©tricas

Implementa m√©tricas de clasificaci√≥n:

```typescript
export interface ClassificationMetrics {
  accuracy: number;
  precision: number;
  recall: number;
  f1Score: number;
  confusionMatrix: number[][];
}

export function computeAccuracy(y_true: number[], y_pred: number[]): number {
  // (TP + TN) / total
}

export function computeConfusionMatrix(
  y_true: number[],
  y_pred: number[]
): number[][] {
  // [[TN, FP], [FN, TP]]
}

export function computeMetrics(
  y_true: number[],
  y_pred: number[]
): ClassificationMetrics {
  // Calcula todas las m√©tricas
}
```

### Parte 6: Clasificaci√≥n Multiclase (Bonus)

Implementa One-vs-Rest:

```typescript
export interface MulticlassModel {
  models: LogisticRegressionModel[];
  classes: number[];
}

export function fitOneVsRest(
  X: number[][],
  y: number[],
  config: TrainingConfig
): MulticlassModel {
  // Entrenar un modelo por cada clase
}

export function predictMulticlass(
  X: number[][],
  model: MulticlassModel
): number[] {
  // Retorna la clase con mayor probabilidad
}
```

### Parte 7: Decision Boundary (Bonus)

Implementa funci√≥n para visualizar l√≠mite de decisi√≥n:

```typescript
export function getDecisionBoundary(
  model: LogisticRegressionModel,
  x1_range: [number, number],
  num_points: number = 100
): number[][] {
  // Retorna puntos (x1, x2) donde Œ∏0 + Œ∏1√óx1 + Œ∏2√óx2 = 0
}
```

---

## ‚úÖ Resultado Esperado

Al finalizar, deber√≠as poder:

1. ‚úÖ Implementar regresi√≥n log√≠stica binaria
2. ‚úÖ Calcular probabilidades y hacer predicciones
3. ‚úÖ Usar cross-entropy como funci√≥n de costo
4. ‚úÖ Calcular accuracy, precision, recall, F1
5. ‚úÖ Crear confusion matrix
6. ‚úÖ Implementar One-vs-Rest para multiclase
7. ‚úÖ Entender el decision boundary

---

## üß™ Tests

Ejecuta los tests para verificar tu implementaci√≥n:

```bash
npm test 07-clasificacion-logistica
```

Los tests verificar√°n:
- Sigmoid funciona correctamente
- Cross-entropy se calcula bien
- Modelo converge
- Predicciones son precisas
- M√©tricas son correctas
- Multiclase funciona

---

## üí° Consejos

1. **Estabilidad num√©rica**: Limita valores de sigmoid para evitar log(0)
2. **Normaliza features**: Mejora convergencia
3. **Learning rate**: 0.01 - 0.1 suele funcionar bien
4. **Regularizaci√≥n**: Usa Œª=1 como punto de partida
5. **Threshold**: Ajusta seg√∫n costo de FP vs FN
6. **Clases desbalanceadas**: Usa F1-score en lugar de accuracy
7. **Inicializaci√≥n**: theta = 0 funciona bien (a diferencia de redes neuronales)

---

## üéì Conceptos Clave

- **Logit**: Logaritmo de odds ‚Üí `log(p/(1-p)) = Œ∏^T√óx`
- **Odds**: Raz√≥n de probabilidades ‚Üí `p/(1-p)`
- **Log-likelihood**: Maximizar log-likelihood = minimizar cross-entropy
- **Maximum Likelihood Estimation (MLE)**: Base te√≥rica de logistic regression
- **Convexidad**: Cross-entropy + sigmoid es convexa ‚Üí convergencia garantizada
- **Separabilidad lineal**: Clases deben ser linealmente separables
- **Polynomial features**: A√±adir x1¬≤, x1√óx2 para boundaries no lineales

---

## üìä Comparaci√≥n: Regresi√≥n Lineal vs Log√≠stica

| Aspecto | Regresi√≥n Lineal | Regresi√≥n Log√≠stica |
|---------|------------------|---------------------|
| Tarea | Regresi√≥n | Clasificaci√≥n |
| Output | Continuo (-‚àû, +‚àû) | Probabilidad [0, 1] |
| Funci√≥n | h(x) = Œ∏^T√óx | h(x) = œÉ(Œ∏^T√óx) |
| Costo | MSE | Cross-Entropy |
| Gradiente | (1/m)X^T(≈∑-y) | (1/m)X^T(œÉ(XŒ∏)-y) |
| Convexidad | Convexa | Convexa |
| Interpretaci√≥n | Valor predicho | Probabilidad |

---

## üìö Recursos

- [Logistic Regression - Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [Cross-Entropy Loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)
- [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)
- [Softmax Regression](https://deeplearning.ai/ai-notes/optimization/)

---

**¬°Comienza implementando en `logistic-regression.ts`!** üöÄ
