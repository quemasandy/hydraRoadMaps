# Ejercicio 09: Decision Trees

**Objetivo:** Implementar √°rboles de decisi√≥n desde cero usando el algoritmo CART con Gini impurity e information gain.

## üìñ Teor√≠a

### ¬øQu√© son los Decision Trees?

√Årboles de decisi√≥n son modelos que aprenden reglas de decisi√≥n a partir de datos.

**Usos:**
- **Clasificaci√≥n**: Predecir categor√≠as
- **Regresi√≥n**: Predecir valores continuos
- **Interpretabilidad**: F√°ciles de visualizar y explicar
- **Base para ensembles**: Random Forest, Gradient Boosting

### Estructura del √Årbol

```
           [Feature1 <= 5]
          /              \
    [Feature2 <= 3]     [Class: B]
      /         \
 [Class: A]   [Class: C]
```

**Componentes:**
- **Nodo ra√≠z**: Primera decisi√≥n
- **Nodos internos**: Preguntas sobre features
- **Hojas**: Predicciones finales
- **Splits**: Divisiones de datos

### Algoritmo CART

**Classification And Regression Trees**

**Proceso:**
```
1. Empezar con todos los datos en la ra√≠z
2. Para cada feature y threshold:
   - Dividir datos en izquierda/derecha
   - Calcular impureza de la divisi√≥n
3. Elegir mejor split (menor impureza)
4. Recursivamente aplicar a nodos hijos
5. Parar cuando:
   - Todos los puntos son de la misma clase
   - Profundidad m√°xima alcanzada
   - Muy pocos puntos en el nodo
```

### Gini Impurity

Mide qu√© tan "impura" es una divisi√≥n:

```
Gini = 1 - Œ£(pi)¬≤
```

Donde `pi` es la proporci√≥n de clase `i`.

**Ejemplos:**
```
[100% clase A]: Gini = 1 - 1¬≤ = 0 (puro)
[50% A, 50% B]: Gini = 1 - (0.5¬≤ + 0.5¬≤) = 0.5 (impuro)
[33% A, 33% B, 33% C]: Gini = 1 - 3√ó(0.33)¬≤ = 0.67 (muy impuro)
```

**Gini del Split:**
```
Gini_split = (n_left/n_total) √ó Gini_left + (n_right/n_total) √ó Gini_right
```

**Objetivo:** Minimizar Gini del split.

### Information Gain (Entropy)

Alternativa a Gini usando entrop√≠a:

```
Entropy = -Œ£(pi √ó log2(pi))
```

**Information Gain:**
```
IG = Entropy_parent - (weighted avg of children entropy)
```

**Objetivo:** Maximizar information gain.

### Comparaci√≥n: Gini vs Entropy

| Aspecto | Gini | Entropy |
|---------|------|---------|
| C√°lculo | M√°s r√°pido | M√°s lento (log) |
| Rango | [0, 0.5] | [0, 1] |
| Resultados | Muy similares | Muy similares |
| Uso | sklearn default | Informaci√≥n te√≥rica |

**Recomendaci√≥n:** Usar Gini (m√°s r√°pido, resultados similares).

### Overfitting

√Årboles profundos memorizan datos:

**S√≠ntomas:**
- 100% accuracy en train, baja en test
- √Årbol muy profundo
- Hojas con muy pocos puntos

**Soluciones:**
1. **max_depth**: Limitar profundidad
2. **min_samples_split**: M√≠nimo de puntos para dividir
3. **min_samples_leaf**: M√≠nimo de puntos por hoja
4. **Pruning**: Podar ramas despu√©s
5. **Ensembles**: Random Forest

---

## üéØ Escenario

Predecir si un cliente comprar√° un producto:

```
Edad | Salario | Compra
25   | 40k     | No
30   | 60k     | No
35   | 80k     | S√≠
40   | 90k     | S√≠
45   | 110k    | S√≠
```

**√Årbol aprendido:**
```
Salario <= 70k?
‚îú‚îÄ S√≠ ‚Üí No
‚îî‚îÄ No ‚Üí S√≠
```

---

## üìù Instrucciones

### Parte 1: Estructura del √Årbol

```typescript
export interface TreeNode {
  featureIndex?: number;
  threshold?: number;
  left?: TreeNode;
  right?: TreeNode;
  value?: number; // Para hojas: clase predicha
  samples?: number;
  gini?: number;
}

export interface DecisionTreeConfig {
  maxDepth?: number;
  minSamplesSplit?: number;
  minSamplesLeaf?: number;
  criterion?: 'gini' | 'entropy';
}
```

### Parte 2: Implementar √Årbol

```typescript
export class DecisionTreeClassifier {
  private root?: TreeNode;
  private config: DecisionTreeConfig;

  constructor(config: DecisionTreeConfig = {}) {
    this.config = {
      maxDepth: 10,
      minSamplesSplit: 2,
      minSamplesLeaf: 1,
      criterion: 'gini',
      ...config,
    };
  }

  fit(X: number[][], y: number[]): void;
  predict(X: number[][]): number[];
  predictOne(x: number[]): number;
}
```

### Parte 3: Funciones de Impureza

```typescript
export function computeGini(y: number[]): number;
export function computeEntropy(y: number[]): number;
export function computeInformationGain(
  y_parent: number[],
  y_left: number[],
  y_right: number[]
): number;
```

### Parte 4: Feature Importance

```typescript
export function computeFeatureImportance(
  tree: DecisionTreeClassifier,
  n_features: number
): number[];
```

---

## ‚úÖ Resultado Esperado

1. ‚úÖ Implementar √°rbol de decisi√≥n con CART
2. ‚úÖ Calcular Gini impurity
3. ‚úÖ Calcular Entropy e Information Gain
4. ‚úÖ Manejar overfitting con hiperpar√°metros
5. ‚úÖ Hacer predicciones
6. ‚úÖ Calcular feature importance

---

## üß™ Tests

```bash
npm test 09-decision-trees
```

---

## üí° Consejos

1. **max_depth=5**: Buen punto de partida
2. **Normalizar no es necesario**: √Årboles son invariantes a escala
3. **Categorical features**: Convertir a one-hot encoding
4. **Visualiza √°rbol**: Ayuda a entender decisiones
5. **Compara con Random Forest**: Suele ser mejor

---

## üìö Recursos

- [Decision Trees - sklearn](https://scikit-learn.org/stable/modules/tree.html)
- [CART Algorithm](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)
- [Gini vs Entropy](https://quantdare.com/decision-trees-gini-vs-entropy/)

---

**¬°Comienza implementando en `decision-tree.ts`!** üöÄ
