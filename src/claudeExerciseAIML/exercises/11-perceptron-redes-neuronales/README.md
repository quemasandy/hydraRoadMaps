# Ejercicio 11: PerceptrÃ³n y Redes Neuronales

**Objetivo:** Implementar desde cero un perceptrÃ³n y redes neuronales Multi-Layer Perceptron (MLP) con diferentes funciones de activaciÃ³n.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© es un PerceptrÃ³n?

**El modelo mÃ¡s simple de neurona artificial** (1958, Frank Rosenblatt)

**Idea central:** Una funciÃ³n que toma entradas, las pondera, suma y decide.

```
      xâ‚ â”€â”€wâ‚â”€â”€â”
      xâ‚‚ â”€â”€wâ‚‚â”€â”€â”¤
      xâ‚ƒ â”€â”€wâ‚ƒâ”€â”€â”œâ”€â”€â†’ Î£(wixi + b) â”€â”€â†’ Ïƒ(z) â”€â”€â†’ Å·
       ...      â”‚
      xâ‚™ â”€â”€wâ‚™â”€â”€â”˜
         bias b
```

**EcuaciÃ³n matemÃ¡tica:**
```
z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
Å· = Ïƒ(z)
```

Donde:
- **x**: Features de entrada
- **w**: Pesos (weights) - importancia de cada feature
- **b**: Bias - umbral de activaciÃ³n
- **z**: Suma ponderada (weighted sum)
- **Ïƒ**: FunciÃ³n de activaciÃ³n
- **Å·**: PredicciÃ³n final

**Â¿CÃ³mo aprende?**
Ajusta pesos w y bias b para minimizar error.

**LimitaciÃ³n:** Solo puede clasificar datos **linealmente separables**.

### Funciones de ActivaciÃ³n

**Â¿Por quÃ© las necesitamos?**
Sin funciones no-lineales, mÃºltiples capas = 1 capa lineal.
Las activaciones introducen **no-linealidad**.

#### 1. Sigmoid (LogÃ­stica)
```
Ïƒ(z) = 1 / (1 + e^(-z))

Rango: (0, 1)
Derivada: Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))
```

**Ventajas:**
- Salida interpretable como probabilidad
- Suave y diferenciable

**Desventajas:**
- **Vanishing gradient**: Derivada muy pequeÃ±a en extremos
- Salida no centrada en 0 (afecta convergencia)

#### 2. Tanh (Tangente HiperbÃ³lica)
```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))

Rango: (-1, 1)
Derivada: tanh'(z) = 1 - tanhÂ²(z)
```

**Ventajas:**
- Centrada en 0 (mejor que sigmoid)
- Convergencia mÃ¡s rÃ¡pida

**Desventajas:**
- TambiÃ©n sufre vanishing gradient

#### 3. ReLU (Rectified Linear Unit)
```
ReLU(z) = max(0, z)

Rango: [0, âˆ)
Derivada: ReLU'(z) = { 1 si z > 0, 0 si z â‰¤ 0 }
```

**Ventajas:**
- **NO vanishing gradient** para z > 0
- Computacionalmente eficiente
- Convergencia ~6x mÃ¡s rÃ¡pida que sigmoid/tanh

**Desventajas:**
- **Dying ReLU**: Neuronas pueden "morir" (output=0 siempre)

#### 4. Leaky ReLU
```
LeakyReLU(z) = max(Î±z, z)  donde Î± = 0.01 tÃ­picamente

Rango: (-âˆ, âˆ)
Derivada: { 1 si z > 0, Î± si z â‰¤ 0 }
```

**Ventajas:**
- Resuelve dying ReLU
- Permite gradiente pequeÃ±o para z < 0

**RecomendaciÃ³n moderna:**
- **Capas ocultas:** ReLU o variantes (Leaky ReLU, ELU, GELU)
- **Salida binaria:** Sigmoid
- **Salida multiclase:** Softmax
- **Salida regresiÃ³n:** Lineal (sin activaciÃ³n)

### Multi-Layer Perceptron (MLP)

**Red neuronal feedforward** con mÃºltiples capas.

**Arquitectura:**
```
Input Layer â†’ Hidden Layer(s) â†’ Output Layer

Ejemplo 3-4-2:
  xâ‚   â•”â•â•â•â•—
  xâ‚‚ â†’â†’â•‘ hâ‚â•‘â†’â†’â•”â•â•â•â•—â†’ yâ‚
  xâ‚ƒ   â•‘ hâ‚‚â•‘  â•‘outâ•‘
       â•‘ hâ‚ƒâ•‘â†’â†’â•šâ•â•â•â•â†’ yâ‚‚
       â•‘ hâ‚„â•‘
       â•šâ•â•â•â•
```

**Forward Pass (PropagaciÃ³n hacia adelante):**
```
1. Input â†’ Hidden Layer:
   h = Ïƒ(Wâ‚ Ã— x + bâ‚)

2. Hidden â†’ Output Layer:
   Å· = Ïƒ(Wâ‚‚ Ã— h + bâ‚‚)
```

Donde:
- Wâ‚: Matriz de pesos input-to-hidden (hidden_size Ã— input_size)
- bâ‚: Vector bias para hidden layer
- Wâ‚‚: Matriz de pesos hidden-to-output (output_size Ã— hidden_size)
- bâ‚‚: Vector bias para output layer

**Â¿Por quÃ© mÃºltiples capas?**
- 1 hidden layer: Puede aproximar cualquier funciÃ³n continua (Universal Approximation Theorem)
- MÃ¡s capas: Aprenden representaciones jerÃ¡rquicas
  - Capa 1: Features bÃ¡sicos
  - Capa 2: Combinaciones de features
  - Capa 3: Conceptos mÃ¡s abstractos

**Trade-offs:**
- MÃ¡s neuronas/capas = MÃ¡s capacidad pero mÃ¡s overfitting
- Pocas neuronas = Underfitting
- **Regla de oro:** Empezar simple, aumentar complejidad segÃºn necesidad

### InicializaciÃ³n de Pesos

**Â¡IMPORTANTE!** No inicializar con ceros â†’ todas neuronas aprenden lo mismo.

**Xavier/Glorot Initialization:**
```
w ~ Uniform(-âˆš(6/(náµ¢â‚™ + nâ‚’áµ¤â‚œ)), âˆš(6/(náµ¢â‚™ + nâ‚’áµ¤â‚œ)))

o Normal(0, âˆš(2/(náµ¢â‚™ + nâ‚’áµ¤â‚œ)))
```

**He Initialization** (para ReLU):
```
w ~ Normal(0, âˆš(2/náµ¢â‚™))
```

---

## ğŸ¯ Escenario

**Problema 1:** ClasificaciÃ³n binaria de flores Iris (Setosa vs No-Setosa)

```
Sepal Length | Sepal Width | Clase
5.1          | 3.5         | Setosa
6.2          | 2.9         | Versicolor
```

**Problema 2:** ClasificaciÃ³n multiclase de dÃ­gitos escritos a mano

```
Pixels de 8x8 â†’ Red Neuronal â†’ DÃ­gito (0-9)
```

---

## ğŸ“ Instrucciones

### Parte 1: Funciones de ActivaciÃ³n

```typescript
export function sigmoid(z: number | number[]): number | number[];
export function sigmoidDerivative(z: number | number[]): number | number[];

export function tanh(z: number | number[]): number | number[];
export function tanhDerivative(z: number | number[]): number | number[];

export function relu(z: number | number[]): number | number[];
export function reluDerivative(z: number | number[]): number | number[];

export function leakyRelu(z: number | number[], alpha?: number): number | number[];
export function leakyReluDerivative(z: number | number[], alpha?: number): number | number[];
```

### Parte 2: PerceptrÃ³n Simple

```typescript
export class Perceptron {
  private weights: number[] = [];
  private bias: number = 0;
  private learningRate: number;

  constructor(learningRate: number = 0.01) {
    this.learningRate = learningRate;
  }

  fit(X: number[][], y: number[], epochs: number = 100): void {
    // 1. Inicializar pesos
    // 2. Para cada Ã©poca:
    //    - Para cada ejemplo:
    //      a. Calcular predicciÃ³n: Å· = Ïƒ(wÂ·x + b)
    //      b. Calcular error: e = y - Å·
    //      c. Actualizar pesos: w = w + Î· Ã— e Ã— x
    //      d. Actualizar bias: b = b + Î· Ã— e
  }

  predict(X: number[][]): number[];
}
```

### Parte 3: Multi-Layer Perceptron

```typescript
export interface MLPConfig {
  inputSize: number;
  hiddenSize: number;
  outputSize: number;
  activation?: 'sigmoid' | 'tanh' | 'relu' | 'leaky_relu';
  learningRate?: number;
  weightInit?: 'random' | 'xavier' | 'he';
}

export class MultiLayerPerceptron {
  private W1: number[][] = [];  // Input â†’ Hidden
  private b1: number[] = [];
  private W2: number[][] = [];  // Hidden â†’ Output
  private b2: number[] = [];

  constructor(config: MLPConfig) {
    // Inicializar pesos y bias
  }

  forward(X: number[][]): {
    hidden: number[][];
    output: number[][];
  } {
    // 1. Hidden layer: h = Ïƒ(Wâ‚ Ã— X + bâ‚)
    // 2. Output layer: Å· = Ïƒ(Wâ‚‚ Ã— h + bâ‚‚)
  }

  predict(X: number[][]): number[];
  predictProba(X: number[][]): number[][];
}
```

### Parte 4: Utilidades

```typescript
export function initializeWeights(
  rows: number,
  cols: number,
  method?: 'random' | 'xavier' | 'he'
): number[][];

export function dotProduct(a: number[], b: number[]): number;

export function matrixMultiply(A: number[][], B: number[][]): number[][];

export function addBias(X: number[][]): number[][];
```

---

## âœ… Resultado Esperado

1. âœ… Implementar 4 funciones de activaciÃ³n + derivadas
2. âœ… PerceptrÃ³n simple con entrenamiento
3. âœ… MLP con forward pass
4. âœ… Diferentes mÃ©todos de inicializaciÃ³n de pesos
5. âœ… Predicciones binarias y probabilÃ­sticas
6. âœ… Comparar activaciones en problemas reales

---

## ğŸ§ª Tests

```bash
npm test 11-perceptron-redes-neuronales
```

---

## ğŸ’¡ Consejos

1. **Sigmoid:** Solo para output layer en clasificaciÃ³n binaria
2. **ReLU:** Default para hidden layers (rÃ¡pido, efectivo)
3. **Leaky ReLU:** Si ves "dying ReLU" (neuronas muertas)
4. **Tanh:** Alternativa a sigmoid, mejor centrado
5. **Hidden size:** Empieza con ~64-128 neuronas
6. **InicializaciÃ³n:** Xavier para sigmoid/tanh, He para ReLU
7. **Learning rate:** 0.01 es buen punto de partida

---

## ğŸ“Š MatemÃ¡ticas Clave

**Forward Pass (2 capas):**
```
Input (x) â†’ Hidden (h) â†’ Output (Å·)

h = Ïƒâ‚(Wâ‚ Ã— x + bâ‚)    â† ActivaciÃ³n en hidden layer
Å· = Ïƒâ‚‚(Wâ‚‚ Ã— h + bâ‚‚)    â† ActivaciÃ³n en output layer
```

**Dimensiones:**
```
x:  (m, n)      m ejemplos, n features
Wâ‚: (h, n)      h neuronas hidden, n inputs
bâ‚: (h,)        bias para cada neurona hidden
h:  (m, h)      activaciones hidden layer
Wâ‚‚: (k, h)      k outputs, h hidden
bâ‚‚: (k,)        bias para cada output
Å·:  (m, k)      predicciones finales
```

---

## ğŸ“š Recursos

- [Neural Networks - 3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk)
- [Activation Functions - ML Glossary](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)
- [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
- [Weight Initialization](https://paperswithcode.com/method/xavier-initialization)

---

**Â¡Comienza implementando las funciones de activaciÃ³n en `neural-network.ts`!** ğŸ§ 
