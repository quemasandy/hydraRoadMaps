# Ejercicio 15: Transformers y Attention

**Objetivo:** Implementar mecanismos de atenci√≥n y componentes b√°sicos de Transformers, la arquitectura que revolucion√≥ NLP y m√°s all√°.

## üìñ Teor√≠a

### ¬øQu√© es Attention?

**Mecanismo que permite al modelo "enfocarse" en partes relevantes de la entrada.**

**Problema que resuelve:**
- RNN/LSTM: Toda la informaci√≥n debe pasar por hidden state (cuello de botella)
- Secuencias largas: Informaci√≥n distante se pierde
- Procesamiento secuencial: No se puede paralelizar

**Soluci√≥n Attention:**
Acceso directo a cualquier parte de la entrada, con pesos aprendidos.

### Analog√≠a: B√∫squeda en Base de Datos

```
Query (Q): "¬øQu√© busco?"
Key (K):   "¬øQu√© tengo?"
Value (V): "Contenido real"

Attention: Encontrar Keys similares a Query,
           luego devolver Values correspondientes.
```

### Atenci√≥n (Attention Mechanism)

**Ecuaci√≥n b√°sica:**
```
Attention(Q, K, V) = softmax(Q √ó K·µÄ / ‚àödk) √ó V
```

**Paso a paso:**
```
1. Similarity scores: S = Q √ó K·µÄ
   "¬øQu√© tan relacionados est√°n query y keys?"

2. Scaling: S_scaled = S / ‚àödk
   "Normalizar para estabilidad num√©rica"

3. Attention weights: Œ± = softmax(S_scaled)
   "Convertir a probabilidades"

4. Weighted sum: Output = Œ± √ó V
   "Combinar values seg√∫n importancia"
```

**Ejemplo num√©rico:**
```
Secuencia: "El gato come pescado"

Para palabra "come":
Query: [0.2, 0.8]

Keys:
El:      [0.1, 0.3]
gato:    [0.4, 0.7]
come:    [0.2, 0.8]
pescado: [0.9, 0.1]

Scores = Q ¬∑ K·µÄ:
El:      0.2√ó0.1 + 0.8√ó0.3 = 0.26
gato:    0.2√ó0.4 + 0.8√ó0.7 = 0.64
come:    0.2√ó0.2 + 0.8√ó0.8 = 0.68
pescado: 0.2√ó0.9 + 0.8√ó0.1 = 0.26

Weights = softmax([0.26, 0.64, 0.68, 0.26]):
‚âà [0.15, 0.22, 0.23, 0.15]

Output = weighted sum of Values
(presta m√°s atenci√≥n a "come" y "gato")
```

### Self-Attention

**Atenci√≥n sobre la misma secuencia.**

```
Input: "El gato come pescado"

Para cada palabra:
  Q = W_Q √ó word
  K = W_K √ó word
  V = W_V √ó word

Cada palabra puede atender a cualquier otra palabra.
```

**Ventajas:**
- Captura relaciones entre cualquier par de palabras
- Paralelizable (no secuencial como RNN)
- No limitado por distancia

**Ejemplo: Resoluci√≥n de correferencias**
```
"El gato vio al perro. √âl estaba feliz."

"√âl" atiende fuertemente a "gato" (no "perro")
gracias a self-attention.
```

### Multi-Head Attention

**M√∫ltiples atenciones en paralelo.**

```
Head 1: Captura sintaxis
Head 2: Captura sem√°ntica
Head 3: Captura dependencias a largo plazo
...
Head h: Otro aspecto

Output = concat(Head‚ÇÅ, Head‚ÇÇ, ..., Head‚Çï) √ó W_O
```

**Ecuaci√≥n:**
```
MultiHead(Q, K, V) = Concat(head‚ÇÅ, ..., head‚Çï) √ó W_O

donde:
head·µ¢ = Attention(Q√óW_Q‚Å±, K√óW_K‚Å±, V√óW_V‚Å±)
```

**Ventajas:**
- Diferentes heads aprenden diferentes patrones
- M√°s expresivo que single-head
- Est√°ndar: 8-12 heads

### Arquitectura Transformer

**Componentes principales:**

```
Input
  ‚Üì
Positional Encoding (a√±adir info de posici√≥n)
  ‚Üì
Multi-Head Self-Attention
  ‚Üì
Add & Norm (residual connection + layer norm)
  ‚Üì
Feed-Forward Network
  ‚Üì
Add & Norm
  ‚Üì
Output
```

**Encoder-Decoder:**
```
Input ‚Üí Encoder ‚Üí Decoder ‚Üí Output

Encoder: Stack de N capas (self-attention + FFN)
Decoder: Stack de N capas (self-attention + cross-attention + FFN)
```

### Positional Encoding

**Problema:** Attention no tiene noci√≥n de orden.

**Soluci√≥n:** A√±adir informaci√≥n de posici√≥n.

**Sinusoidal encoding:**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))

donde:
- pos: posici√≥n en secuencia
- i: dimensi√≥n
- d: dimensi√≥n del modelo
```

**Propiedades:**
- Diferentes frecuencias para diferentes dimensiones
- Permite extrapolar a secuencias m√°s largas
- Patrones distinguibles para cada posici√≥n

### Masked Attention

**Para Decoder: No ver el futuro.**

```
Durante entrenamiento de "El gato come":

Para predecir "gato": Solo ve "El"
Para predecir "come": Ve "El gato"
Para predecir <EOS>:  Ve "El gato come"
```

**Implementaci√≥n:** Mask infinito en futuro antes de softmax.

```
Scores:
     El  gato  come
El   0.5  0.3   0.2
gato 0.4  0.6   0.8
come 0.2  0.1   0.3

Masked (durante predicci√≥n de "gato"):
     El  gato  come
El   0.5  -‚àû    -‚àû
gato 0.4  0.6   -‚àû
come 0.2  0.1   0.3

Despu√©s de softmax, -‚àû ‚Üí 0
```

### ¬øPor qu√© Transformers son tan buenos?

**1. Paralelizaci√≥n:**
- RNN: Secuencial (lento)
- Transformer: Todo en paralelo (r√°pido)

**2. Long-range dependencies:**
- RNN: Gradientes se desvanecen
- Transformer: Conexi√≥n directa v√≠a attention

**3. Flexibilidad:**
- BERT: Encoder-only (comprensi√≥n)
- GPT: Decoder-only (generaci√≥n)
- T5: Encoder-Decoder (traducci√≥n)

**4. Escalabilidad:**
- M√°s datos + m√°s par√°metros = mejor
- GPT-3: 175B par√°metros
- Palm: 540B par√°metros

---

## üéØ Escenario

**Problema:** Traducci√≥n o clasificaci√≥n de texto

```
Input:  "I love machine learning"
Tokens: [1, 234, 567, 891]
Embeddings: 4 √ó 512

‚Üí Self-Attention ‚Üí Captura relaciones
‚Üí FFN ‚Üí Transformaciones
‚Üí Output ‚Üí Clasificaci√≥n o siguiente palabra
```

---

## üìù Instrucciones

### Parte 1: Scaled Dot-Product Attention

```typescript
export function scaledDotProductAttention(
  Q: number[][],
  K: number[][],
  V: number[][],
  mask?: number[][]
): {
  output: number[][];
  attentionWeights: number[][];
};
```

### Parte 2: Multi-Head Attention

```typescript
export class MultiHeadAttention {
  constructor(
    dModel: number,
    numHeads: number
  ) {
    // Inicializar W_Q, W_K, W_V para cada head
    // Inicializar W_O para output
  }

  forward(
    Q: number[][],
    K: number[][],
    V: number[][],
    mask?: number[][]
  ): number[][];
}
```

### Parte 3: Positional Encoding

```typescript
export function positionalEncoding(
  sequenceLength: number,
  dModel: number
): number[][];

export function addPositionalEncoding(
  embeddings: number[][],
  encodings: number[][]
): number[][];
```

### Parte 4: Transformer Encoder Layer

```typescript
export class TransformerEncoderLayer {
  constructor(
    dModel: number,
    numHeads: number,
    dFF: number
  ) {
    // Multi-head attention
    // Feed-forward network
    // Layer normalization
  }

  forward(x: number[][]): number[][];
}
```

---

## ‚úÖ Resultado Esperado

1. ‚úÖ Scaled dot-product attention desde cero
2. ‚úÖ Multi-head attention con m√∫ltiples heads
3. ‚úÖ Positional encoding sinusoidal
4. ‚úÖ Masking para decoder
5. ‚úÖ Componentes de Transformer encoder
6. ‚úÖ Visualizar attention weights

---

## üß™ Tests

```bash
npm test 15-transformers-attention
```

---

## üí° Consejos

1. **dModel:** 512 (BERT-base), 768 (BERT-large), 1024 (GPT-2)
2. **numHeads:** 8-12 es est√°ndar
3. **Scaling:** Divisi√≥n por ‚àödk previene softmax saturation
4. **Dropout:** Aplicar en attention weights y FFN
5. **Layer Norm:** Antes o despu√©s (pre-norm vs post-norm)
6. **Residual:** Siempre a√±adir conexiones residuales

---

## üìä Matem√°ticas Detalladas

**Attention paso a paso:**
```
Dado:
Q: (seq_len, d_k)  Query
K: (seq_len, d_k)  Key
V: (seq_len, d_v)  Value

Paso 1: Scores
S = Q √ó K·µÄ
S: (seq_len, seq_len)

Paso 2: Scale
S_scaled = S / ‚àöd_k

Paso 3: Mask (opcional)
S_masked = S_scaled + mask
donde mask[i][j] = -‚àû si position j no debe atender a i

Paso 4: Softmax
Œ± = softmax(S_masked, dim=-1)
Œ±: (seq_len, seq_len)  # suma fila = 1

Paso 5: Apply to values
Output = Œ± √ó V
Output: (seq_len, d_v)
```

**Multi-Head dimensiones:**
```
Input: (seq_len, d_model)

Para cada head i:
  Q_i = Input √ó W_Q^i    W_Q^i: (d_model, d_k)
  K_i = Input √ó W_K^i    W_K^i: (d_model, d_k)
  V_i = Input √ó W_V^i    W_V^i: (d_model, d_v)

  head_i = Attention(Q_i, K_i, V_i)
  head_i: (seq_len, d_v)

MultiHead = Concat(head_1, ..., head_h) √ó W_O
          : (seq_len, h√ód_v) √ó (h√ód_v, d_model)
          = (seq_len, d_model)

T√≠picamente: d_k = d_v = d_model / h
```

---

## üìö Recursos

- [Attention is All You Need (Paper)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
- [Attention Mechanisms (Distill)](https://distill.pub/2016/augmented-rnns/)

---

**¬°Comienza implementando scaled dot-product attention en `transformer.ts`!** ü§ñ
