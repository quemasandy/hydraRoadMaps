# Ejercicio 16: Transfer Learning

**Objetivo:** Implementar tÃ©cnicas de Transfer Learning, incluyendo feature extraction y fine-tuning, aprovechando conocimiento pre-entrenado para nuevas tareas.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© es Transfer Learning?

**Reutilizar conocimiento aprendido de una tarea para resolver otra tarea relacionada.**

**Problema que resuelve:**
- Datasets pequeÃ±os: No hay suficientes datos para entrenar desde cero
- Costo computacional: Entrenar redes profundas es muy costoso
- Tiempo: Entrenar puede tomar dÃ­as/semanas
- GeneralizaciÃ³n: Modelos pequeÃ±os no generalizan bien

**SoluciÃ³n Transfer Learning:**
Usar modelo pre-entrenado en dataset grande (ImageNet, Wikipedia) y adaptarlo a tu tarea especÃ­fica.

### AnalogÃ­a: Aprender un Nuevo Idioma

```
Aprender espaÃ±ol conociendo inglÃ©s:
âœ“ Ya sabes gramÃ¡tica (estructura)
âœ“ Ya sabes vocabulario relacionado (cognados)
âœ— Solo necesitas aprender diferencias especÃ­ficas

Transfer Learning:
âœ“ Red pre-entrenada ya sabe detectar bordes, formas, texturas
âœ“ Solo necesitas entrenar capas finales para tu tarea especÃ­fica
```

### Estrategias de Transfer Learning

**1. Feature Extraction (ExtracciÃ³n de Features)**

```
Modelo Pre-entrenado:
Layers 1-10 (frozen) â†’ Extraen features generales
    â†“
Nueva capa (trainable) â†’ Clasificador especÃ­fico

Ventajas:
âœ“ Muy rÃ¡pido (solo entrenas Ãºltima capa)
âœ“ Menos overfitting (menos parÃ¡metros)
âœ“ Funciona con datasets muy pequeÃ±os

CuÃ¡ndo usar:
- Dataset pequeÃ±o (<1000 ejemplos)
- Tarea muy similar al pre-entrenamiento
- Poco poder computacional
```

**2. Fine-Tuning (Ajuste Fino)**

```
Modelo Pre-entrenado:
Layers 1-5 (frozen) â†’ Features muy generales
Layers 6-10 (trainable) â†’ Features especÃ­ficas
Nueva capa (trainable) â†’ Clasificador

Ventajas:
âœ“ Mejor performance que feature extraction
âœ“ Se adapta mejor a tu tarea especÃ­fica
âœ“ Balancea velocidad y precisiÃ³n

CuÃ¡ndo usar:
- Dataset mediano (1000-10000 ejemplos)
- Tarea relacionada pero no idÃ©ntica
- Suficiente poder computacional
```

**3. Pre-training from Scratch**

```
Todo trainable desde inicio

CuÃ¡ndo usar:
- Dataset muy grande (>100000 ejemplos)
- Tarea muy diferente
- Mucho poder computacional
```

### Arquitectura TÃ­pica

```
Pre-trained Model (ej. ResNet, VGG, BERT):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Layer                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Early Layers (frozen)                â”‚
â”‚   - Detectan bordes, colores         â”‚
â”‚   - Features muy generales           â”‚
â”‚   - Transferible a cualquier tarea   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Middle Layers (frozen o trainable)   â”‚
â”‚   - Detectan formas, texturas        â”‚
â”‚   - Features intermedias             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Late Layers (trainable)              â”‚
â”‚   - Features especÃ­ficas del dataset â”‚
â”‚   - Se adaptan a nueva tarea         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ New Classification Head (trainable)  â”‚
â”‚   - EspecÃ­fico para tu problema      â”‚
â”‚   - NÃºmero de clases de tu dataset   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿QuÃ© Capas Congelar?

**Regla general:**

```
Similitud de tarea vs Dataset size:

Dataset PequeÃ±o + Tarea Similar:
â†’ Congelar TODO excepto Ãºltima capa

Dataset PequeÃ±o + Tarea Diferente:
â†’ Congelar early layers, entrenar middle + last

Dataset Grande + Tarea Similar:
â†’ Fine-tune todas las capas con learning rate bajo

Dataset Grande + Tarea Diferente:
â†’ Fine-tune todo o entrenar desde cero
```

### Learning Rate en Transfer Learning

**Diferentes learning rates por capa:**

```
Early Layers (muy generales):
  lr = 1e-5  (muy bajo, apenas modificar)

Middle Layers:
  lr = 1e-4  (bajo, ajustar suavemente)

New Layers (especÃ­ficos):
  lr = 1e-3  (normal, entrenar activamente)
```

**Por quÃ©:**
- Early layers ya aprendieron features Ãºtiles
- No queremos destruir ese conocimiento
- New layers parten de cero, necesitan aprender mÃ¡s

### Datasets Pre-entrenados Comunes

**VisiÃ³n:**
- ImageNet: 1.4M imÃ¡genes, 1000 clases
- COCO: 330K imÃ¡genes, detecciÃ³n de objetos
- Places365: Reconocimiento de escenas

**NLP:**
- BERT: Pre-entrenado en Wikipedia + BookCorpus
- GPT: Pre-entrenado en WebText
- Word2Vec: Embeddings de palabras

**Audio:**
- AudioSet: 2M clips de audio
- VGGish: Features de audio

### Domain Adaptation

**Cuando source domain â‰  target domain:**

```
Source: ImageNet (fotos naturales)
Target: Rayos X mÃ©dicos

SoluciÃ³n:
1. Pre-train en source
2. Fine-tune con learning rate muy bajo
3. Posiblemente aÃ±adir domain-specific layers
4. Data augmentation especÃ­fica del dominio
```

### Ventajas vs Desventajas

**Ventajas:**
âœ“ Menos datos necesarios
âœ“ Entrenamiento mÃ¡s rÃ¡pido
âœ“ Mejor generalizaciÃ³n
âœ“ Aprovecha conocimiento existente
âœ“ Reduce overfitting

**Desventajas:**
âœ— Modelo base puede ser muy grande
âœ— No siempre funciona (dominios muy diferentes)
âœ— Puede heredar biases del pre-entrenamiento
âœ— Menos flexible que entrenar desde cero

---

## ğŸ¯ Escenario

**Problema:** Clasificar imÃ¡genes mÃ©dicas con solo 100 ejemplos

```
Sin Transfer Learning:
Dataset: 100 imÃ¡genes
Red profunda: 10M parÃ¡metros
Resultado: Overfitting masivo ğŸ˜¢

Con Transfer Learning:
Pre-trained: ResNet en ImageNet
Congelar: Primeras 90% capas
Entrenar: Solo 100K parÃ¡metros
Resultado: 95% accuracy ğŸ‰
```

**Ejemplo PrÃ¡ctico:**

```typescript
// Feature Extraction
const baseModel = loadPretrainedModel('imagenet');
baseModel.freeze(); // Congelar todas las capas

const classifier = new DenseLayer(numClasses);
const model = combine(baseModel, classifier);

model.train(smallDataset);  // Solo entrena classifier

// Fine-Tuning
baseModel.unfreeze(['layer8', 'layer9', 'layer10']);
model.train(smallDataset, { lr: 1e-5 });  // Learning rate bajo
```

---

## ğŸ“ Instrucciones

### Parte 1: Modelo Pre-entrenado Simulado

```typescript
export class PretrainedModel {
  constructor(architecture: string) {
    // Simular pesos pre-entrenados
    // Diferentes arquitecturas: 'simple', 'medium', 'deep'
  }

  extractFeatures(input: number[][]): number[];

  freeze(): void;
  unfreeze(layerNames?: string[]): void;

  getLayerNames(): string[];
  isFrozen(layerName: string): boolean;
}
```

### Parte 2: Feature Extraction

```typescript
export class FeatureExtractor {
  constructor(
    baseModel: PretrainedModel,
    numClasses: number
  ) {
    // Base model (frozen)
    // Nueva capa de clasificaciÃ³n
  }

  train(
    X: number[][][],
    y: number[],
    config: TrainingConfig
  ): TrainingHistory;

  predict(input: number[][]): number;
  predictProba(input: number[][]): number[];
}
```

### Parte 3: Fine-Tuning

```typescript
export class FineTuner {
  constructor(
    baseModel: PretrainedModel,
    numClasses: number,
    layersToUnfreeze: string[]
  ) {
    // Descongelar capas especÃ­ficas
    // Nueva capa de clasificaciÃ³n
  }

  train(
    X: number[][][],
    y: number[],
    config: FineTuningConfig
  ): TrainingHistory;

  // Learning rates diferentes por capa
  setLayerLearningRate(
    layerName: string,
    lr: number
  ): void;
}
```

### Parte 4: Transfer Learning Manager

```typescript
export class TransferLearningManager {
  static recommendStrategy(
    datasetSize: number,
    taskSimilarity: 'low' | 'medium' | 'high'
  ): {
    strategy: 'feature_extraction' | 'fine_tuning' | 'from_scratch';
    layersToFreeze: string[];
    recommendedLR: number;
    reasoning: string;
  };

  static compareStrategies(
    dataset: { X: number[][][]; y: number[] }
  ): {
    featureExtraction: { accuracy: number; time: number };
    fineTuning: { accuracy: number; time: number };
    fromScratch: { accuracy: number; time: number };
  };
}
```

---

## âœ… Resultado Esperado

1. âœ… Modelo pre-entrenado simulado con capas congelables
2. âœ… Feature extraction con base congelada
3. âœ… Fine-tuning con capas selectivas
4. âœ… Learning rates diferenciados por capa
5. âœ… ComparaciÃ³n de estrategias
6. âœ… RecomendaciÃ³n automÃ¡tica de estrategia

---

## ğŸ§ª Tests

```bash
npm test 16-transfer-learning
```

---

## ğŸ’¡ Consejos

1. **Congelar temprano:** Siempre congela early layers primero
2. **Learning rate:** Usa 10-100x menor para capas pre-entrenadas
3. **Warm-up:** Entrena solo la nueva capa primero, luego fine-tune
4. **RegularizaciÃ³n:** Dropout mÃ¡s agresivo en capas nuevas
5. **Batch Normalization:** Cuidado al descongelar (modo train vs eval)
6. **Gradual unfreezing:** Descongela capas progresivamente

---

## ğŸ“Š MatemÃ¡ticas del Fine-Tuning

**ActualizaciÃ³n de pesos:**

```
Para capa congelada:
w_new = w_old  (sin cambios)

Para capa descongelada:
w_new = w_old - lr Ã— âˆ‡L

Para fine-tuning:
w_new = w_old - (lr Ã— decay) Ã— âˆ‡L

donde decay = {
  0.01 para early layers   (casi congeladas)
  0.1  para middle layers  (ajuste suave)
  1.0  para new layers     (entrenamiento normal)
}
```

**Trade-off entre congelar y entrenar:**

```
MÃ¡s capas congeladas:
  + MÃ¡s rÃ¡pido
  + Menos overfitting
  - Menos adaptaciÃ³n a nueva tarea

MÃ¡s capas entrenables:
  + Mejor adaptaciÃ³n
  + Mayor accuracy potencial
  - MÃ¡s riesgo de overfitting
  - MÃ¡s lento
```

---

## ğŸ”„ Flujo TÃ­pico de Transfer Learning

```
1. Cargar modelo pre-entrenado
   â†“
2. Remover Ãºltima capa (clasificador)
   â†“
3. AÃ±adir nueva capa para tu tarea
   â†“
4. Congelar base model
   â†“
5. Entrenar solo nueva capa (pocas epochs)
   â†“
6. [Opcional] Descongelar algunas capas
   â†“
7. [Opcional] Fine-tune con lr bajo
   â†“
8. Evaluar y comparar estrategias
```

---

## ğŸ“š Recursos

- [Transfer Learning Guide (CS231n)](http://cs231n.github.io/transfer-learning/)
- [How transferable are features in deep neural networks?](https://arxiv.org/abs/1411.1792)
- [A Survey on Transfer Learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)
- [Fine-tuning Pre-trained Models](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)

---

**Â¡Comienza implementando el modelo pre-entrenado simulado en `transfer-learning.ts`!** ğŸš€
