# Ejercicio 19: OptimizaciÃ³n de HiperparÃ¡metros

**Objetivo:** Implementar tÃ©cnicas de optimizaciÃ³n de hiperparÃ¡metros incluyendo Grid Search, Random Search y validaciÃ³n cruzada para encontrar la mejor configuraciÃ³n del modelo.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© son HiperparÃ¡metros?

**ParÃ¡metros que se configuran ANTES del entrenamiento y controlan el proceso de aprendizaje.**

**Diferencia con ParÃ¡metros:**
```
ParÃ¡metros (aprendidos):
  - Pesos de red neuronal
  - Coeficientes de regresiÃ³n
  - Centroides de K-Means
  â†’ Se optimizan durante entrenamiento

HiperparÃ¡metros (configurados):
  - Learning rate
  - NÃºmero de capas/neuronas
  - RegularizaciÃ³n (Î»)
  â†’ Se eligen antes de entrenar
```

**Ejemplos por algoritmo:**

```
RegresiÃ³n Lineal:
  - Î» (regularizaciÃ³n L1/L2)
  - Polynomial degree

Ãrboles de DecisiÃ³n:
  - max_depth
  - min_samples_split
  - min_samples_leaf

Redes Neuronales:
  - learning_rate
  - batch_size
  - num_layers
  - neurons_per_layer
  - dropout_rate
  - activation_function

K-Means:
  - k (nÃºmero de clusters)
  - max_iterations

Gradient Boosting:
  - n_estimators
  - max_depth
  - learning_rate
```

### Â¿Por quÃ© Optimizar HiperparÃ¡metros?

**Impacto enorme en performance:**

```
Ejemplo: Red Neuronal

Config A:
  lr=0.001, layers=2, neurons=64
  â†’ Accuracy: 75%

Config B:
  lr=0.01, layers=3, neurons=128
  â†’ Accuracy: 92%

Misma arquitectura, distinta config!
```

**Trade-offs:**
- **Underfitting:** Modelo muy simple (ej. max_depth=1)
- **Overfitting:** Modelo muy complejo (ej. max_depth=âˆ)
- **Optimal:** Balance perfecto

---

## ğŸ¯ Estrategias de BÃºsqueda

### 1. Manual Search

**Probar configuraciones a mano.**

```
Ventajas:
  âœ“ Usa conocimiento del dominio
  âœ“ RÃ¡pido para expertos

Desventajas:
  âœ— Tedioso y lento
  âœ— Depende de intuiciÃ³n
  âœ— No sistemÃ¡tico
```

### 2. Grid Search

**Probar todas las combinaciones de una cuadrÃ­cula.**

```typescript
params = {
  learning_rate: [0.001, 0.01, 0.1],
  num_layers: [2, 3, 4],
  neurons: [32, 64, 128]
}

Grid Search:
  Total configs = 3 Ã— 3 Ã— 3 = 27
  Probar las 27 combinaciones
  Elegir la mejor
```

**Ventajas:**
- SistemÃ¡tico y exhaustivo
- Garantiza encontrar mejor combinaciÃ³n en grid

**Desventajas:**
- ExplosiÃ³n combinatoria (curse of dimensionality)
- Desperdicia recursos en combinaciones malas
- Solo explora valores discretos

**Complejidad:**
```
n parÃ¡metros, k valores cada uno:
Total = k^n configuraciones

Ejemplo:
  5 parÃ¡metros, 10 valores â†’ 10^5 = 100,000 configs!
```

### 3. Random Search

**Probar configuraciones aleatorias.**

```typescript
params = {
  learning_rate: uniform(0.0001, 0.1),
  num_layers: randint(2, 5),
  neurons: randint(16, 256)
}

Random Search:
  Total trials = 50 (configurable)
  Samplear 50 configuraciones aleatorias
  Elegir la mejor
```

**Ventajas:**
- MÃ¡s eficiente que Grid Search
- Explora espacio continuo
- FÃ¡cil de paralel izar

**Desventajas:**
- No garantiza encontrar Ã³ptimo
- Puede duplicar configs similares

**Â¿Por quÃ© funciona mejor?**
```
Grid Search:
  Valor1  Valor2  Valor3
    Ã—       Ã—       Ã—     â†’ 3 valores Ãºnicos por parÃ¡metro

Random Search con 9 trials:
  Valores distribuidos en rango continuo
    â†’ 9 valores Ãºnicos por parÃ¡metro

Para parÃ¡metros importantes, Random explora mejor!
```

### 4. Bayesian Optimization

**Usar modelos probabilÃ­sticos para elegir prÃ³xima configuraciÃ³n.**

```
Proceso:
  1. Probar configuraciÃ³n inicial
  2. Construir modelo (Gaussian Process)
  3. Modelo predice quÃ© config probar siguiente
  4. Actualizar modelo con resultado
  5. Repetir

Acquisition Function:
  - Expected Improvement (EI)
  - Probability of Improvement (PI)
  - Upper Confidence Bound (UCB)
```

**Ventajas:**
- Muy eficiente (pocas evaluaciones)
- Balancea exploration/exploitation
- Estado del arte

**Desventajas:**
- MÃ¡s complejo de implementar
- Overhead de modelado
- DifÃ­cil de paralelizar

---

## ğŸ“Š ValidaciÃ³n Cruzada

**Evaluar modelo de forma robusta.**

### K-Fold Cross-Validation

```
Dataset dividido en K folds:

Fold 1: Test  | Train | Train | Train | Train
Fold 2: Train | Test  | Train | Train | Train
Fold 3: Train | Train | Test  | Train | Train
Fold 4: Train | Train | Train | Test  | Train
Fold 5: Train | Train | Train | Train | Test

Performance = Promedio de los K folds
```

**Ventajas:**
- Usa todos los datos para train y test
- Reduce varianza del estimador
- Detecta overfitting

**TÃ­pico:** K=5 o K=10

### Stratified K-Fold

**Mantiene proporciÃ³n de clases en cada fold.**

```
Dataset desbalanceado:
  Clase 0: 90%
  Clase 1: 10%

K-Fold normal:
  Fold puede tener solo clase 0 â†’ malo

Stratified K-Fold:
  Cada fold tiene 90% clase 0, 10% clase 1 â†’ bien
```

### Train/Validation/Test Split

```
Durante Hyperparameter Search:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Training Set            â”‚ 60%
â”‚  (entrena modelo con cada config)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Validation Set           â”‚ 20%
â”‚  (elige mejor config)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Test Set               â”‚ 20%
â”‚  (evalÃºa modelo final)          â”‚
â”‚  Â¡NO USAR PARA TUNING!          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Proceso:
  1. Grid/Random Search usa Train+Val
  2. Mejor config se entrena en Train+Val
  3. Performance final se evalÃºa en Test
```

---

## ğŸ’¡ Consejos PrÃ¡cticos

### Orden de Importancia

**Empezar con hiperparÃ¡metros mÃ¡s importantes:**

1. **Learning rate** (crucial en casi todo)
2. **Architecture** (capas, neuronas)
3. **RegularizaciÃ³n** (dropout, L2)
4. **Batch size**
5. **Optimizador** (Adam vs SGD)
6. **Activation functions**

### Rangos Comunes

```typescript
learning_rate: [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
                (escala logarÃ­tmica)

batch_size: [16, 32, 64, 128, 256]
            (potencias de 2)

dropout: [0.0, 0.1, 0.2, 0.3, 0.5]
         (0 = no dropout)

weight_decay: [1e-5, 1e-4, 1e-3]
              (regularizaciÃ³n L2)
```

### Estrategia PrÃ¡ctica

**Coarse to Fine:**

```
1. Grid Search grueso:
   lr: [0.001, 0.01, 0.1]
   layers: [2, 4, 6]
   â†’ Mejor: lr=0.01, layers=4

2. Random Search fino:
   lr: uniform(0.005, 0.015)
   layers: [3, 4, 5]
   â†’ Refinar alrededor del mejor

3. Final tuning manual
```

### Early Stopping

**No entrenar epochs completos si modelo es malo.**

```typescript
if (validation_loss_at_epoch_5 > threshold) {
  // Este config es malo, no vale la pena continuar
  abort_training();
  try_next_config();
}
```

**Ahorra muchÃ­simo tiempo.**

---

## ğŸ“ Instrucciones

### Parte 1: Grid Search

```typescript
export class GridSearch {
  constructor(
    paramGrid: Record<string, any[]>,
    model: Model,
    scorer: (y_true: number[], y_pred: number[]) => number
  );

  fit(X: number[][], y: number[]): GridSearchResult;

  getBestParams(): Record<string, any>;
  getBestScore(): number;
}
```

### Parte 2: Random Search

```typescript
export class RandomSearch {
  constructor(
    paramDistributions: Record<string, Distribution>,
    model: Model,
    nIter: number,
    scorer: (y_true: number[], y_pred: number[]) => number
  );

  fit(X: number[][], y: number[]): RandomSearchResult;
}
```

### Parte 3: Cross-Validation

```typescript
export function crossValidate(
  model: Model,
  X: number[][],
  y: number[],
  cv: number,
  stratify?: boolean
): CrossValidationResult;

export function kFoldSplit(
  X: number[][],
  y: number[],
  k: number
): { train: number[]; test: number[] }[];
```

---

## âœ… Resultado Esperado

1. âœ… Grid Search exhaustivo
2. âœ… Random Search eficiente
3. âœ… K-Fold Cross-Validation
4. âœ… Stratified CV para clases desbalanceadas
5. âœ… ComparaciÃ³n de estrategias
6. âœ… VisualizaciÃ³n de resultados

---

## ğŸ§ª Tests

```bash
npm test 19-optimizacion-hyperparametros
```

---

## ğŸ“š Recursos

- [Hyperparameter Optimization (Bergstra & Bengio)](https://www.jmlr.org/papers/v13/bergstra12a.html)
- [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
- [Scikit-learn: Model Selection](https://scikit-learn.org/stable/model_selection.html)

---

**Â¡Comienza implementando Grid Search en `hyperparameter-optimization.ts`!** ğŸ”§
