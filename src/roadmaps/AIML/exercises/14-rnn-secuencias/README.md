# Ejercicio 14: RNN para Secuencias

**Objetivo:** Implementar Recurrent Neural Networks (RNN) y Long Short-Term Memory (LSTM) para procesar datos secuenciales.

## ğŸ“– TeorÃ­a

### Â¿QuÃ© son las RNN?

**Redes neuronales con memoria** diseÃ±adas para secuencias.

**Problema que resuelven:**
- Feedforward networks: No capturan orden temporal
- CNN: Capturan patrones locales pero no dependencias largas
- RNN: Procesan secuencias manteniendo "memoria" del pasado

**Ejemplos de datos secuenciales:**
- Texto: "El gato" â†’ "comiÃ³" â†’ "pescado"
- Series temporales: Temperatura por hora
- Audio: SeÃ±al de voz
- Video: Frames en el tiempo

### Arquitectura RNN

**Estructura bÃ¡sica:**
```
      ht-1 â”€â”€â”€â”€â”€â”€â”
                 â”‚
Input: xt â†’ [RNN Cell] â†’ Output: yt
                 â”‚
                ht â”€â”€â”€â”€â”€â”€â†’
```

**Ecuaciones:**
```
ht = tanh(Whh Ã— ht-1 + Wxh Ã— xt + bh)
yt = Why Ã— ht + by
```

**Desenrollado en el tiempo:**
```
x1 â†’ [Cell] â†’ h1 â†’ [Cell] â†’ h2 â†’ [Cell] â†’ h3
      â†“ y1          â†“ y2          â†“ y3
```

Cada celda comparte los mismos pesos (Whh, Wxh, Why).

### Tipos de Secuencias

**One-to-One:** Input fijo â†’ Output fijo (no es RNN)
```
Imagen â†’ ClasificaciÃ³n
```

**One-to-Many:** Input fijo â†’ Secuencia output
```
Imagen â†’ DescripciÃ³n textual (Image Captioning)
```

**Many-to-One:** Secuencia input â†’ Output fijo
```
Frase â†’ Sentimiento (positivo/negativo)
```

**Many-to-Many (sync):** Secuencia â†’ Secuencia (mismo tamaÃ±o)
```
Video â†’ Clasificar cada frame
```

**Many-to-Many (async):** Secuencia â†’ Secuencia (diferente tamaÃ±o)
```
InglÃ©s â†’ EspaÃ±ol (TraducciÃ³n)
```

### Problema: Vanishing Gradient

**En RNN, gradientes se multiplican a travÃ©s del tiempo:**
```
âˆ‚L/âˆ‚h1 = âˆ‚L/âˆ‚h3 Ã— âˆ‚h3/âˆ‚h2 Ã— âˆ‚h2/âˆ‚h1

Si |âˆ‚ht/âˆ‚ht-1| < 1:
  Gradiente â†’ 0 (vanishing)

Si |âˆ‚ht/âˆ‚ht-1| > 1:
  Gradiente â†’ âˆ (exploding)
```

**Consecuencia:**
RNN simple solo "recuerda" ~10 pasos atrÃ¡s.

**Soluciones:**
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)
- Gradient clipping (para exploding)
- Skip connections

### LSTM (Long Short-Term Memory)

**DiseÃ±ado para capturar dependencias largas.**

**Componentes:**
```
Cell State (Ct): "Memoria a largo plazo"
Hidden State (ht): "Memoria a corto plazo"
```

**Gates (Compuertas):**
```
1. Forget Gate (ft): QuÃ© olvidar de Ct-1
   ft = Ïƒ(Wf Ã— [ht-1, xt] + bf)

2. Input Gate (it): QuÃ© aÃ±adir a Ct
   it = Ïƒ(Wi Ã— [ht-1, xt] + bi)
   CÌƒt = tanh(WC Ã— [ht-1, xt] + bC)

3. Cell State Update:
   Ct = ft âŠ™ Ct-1 + it âŠ™ CÌƒt
   (âŠ™ = element-wise multiplication)

4. Output Gate (ot): QuÃ© parte de Ct mostrar
   ot = Ïƒ(Wo Ã— [ht-1, xt] + bo)
   ht = ot âŠ™ tanh(Ct)
```

**IntuiciÃ³n:**
- Forget gate: "Olvidemos el sujeto anterior"
- Input gate: "Recordemos el nuevo sujeto"
- Output gate: "Mostremos informaciÃ³n relevante ahora"

**Â¿Por quÃ© funciona?**
El cell state Ct fluye directamente con solo multiplicaciones element-wise, permitiendo que gradientes fluyan mejor.

### GRU (Gated Recurrent Unit)

**VersiÃ³n simplificada de LSTM** (menos parÃ¡metros).

**Ecuaciones:**
```
1. Reset Gate:
   rt = Ïƒ(Wr Ã— [ht-1, xt])

2. Update Gate:
   zt = Ïƒ(Wz Ã— [ht-1, xt])

3. Candidate:
   hÌƒt = tanh(W Ã— [rt âŠ™ ht-1, xt])

4. Hidden State:
   ht = (1 - zt) âŠ™ ht-1 + zt âŠ™ hÌƒt
```

**LSTM vs GRU:**
- LSTM: MÃ¡s expresivo, mejor para secuencias largas
- GRU: MÃ¡s simple, mÃ¡s rÃ¡pido, similar performance
- Regla: Empezar con GRU, usar LSTM si necesitas mÃ¡s capacidad

### Bidirectional RNN

**Procesa secuencia en ambas direcciones:**
```
Forward:  h1â†’ h2â†’ h3â†’
Backward: â†h1 â†h2 â†h3

Output: [h_forward, h_backward] concatenados
```

**Ãštil cuando:** Contexto futuro ayuda (ej: traducciÃ³n, NER)
**No Ãºtil cuando:** PredicciÃ³n en tiempo real (no hay futuro disponible)

---

## ğŸ¯ Escenario

**Problema 1:** PredicciÃ³n de series temporales
```
Input: [temp(t-4), temp(t-3), temp(t-2), temp(t-1)]
Output: temp(t)
```

**Problema 2:** ClasificaciÃ³n de sentimiento
```
Input: "Esta pelÃ­cula es increÃ­ble" â†’ [0.1, 0.3, 0.2, 0.8]
Output: Positivo (1)
```

---

## ğŸ“ Instrucciones

### Parte 1: RNN Simple

```typescript
export class SimpleRNN {
  constructor(
    inputSize: number,
    hiddenSize: number,
    outputSize: number
  ) {
    // Inicializar Wxh, Whh, Why
  }

  forward(inputs: number[][]): {
    hiddenStates: number[][];
    outputs: number[][];
  } {
    // Procesar secuencia paso a paso
  }

  predict(inputs: number[][]): number[];
}
```

### Parte 2: LSTM Cell

```typescript
export class LSTMCell {
  forward(
    xt: number[],
    ht_prev: number[],
    Ct_prev: number[]
  ): {
    ht: number[];
    Ct: number[];
  } {
    // 1. Forget gate
    // 2. Input gate
    // 3. Cell state update
    // 4. Output gate
  }
}
```

### Parte 3: LSTM Layer

```typescript
export class LSTM {
  constructor(
    inputSize: number,
    hiddenSize: number,
    outputSize: number
  ) {
    // Inicializar parÃ¡metros para gates
  }

  forward(inputs: number[][]): {
    hiddenStates: number[][];
    cellStates: number[][];
    outputs: number[][];
  };
}
```

---

## âœ… Resultado Esperado

1. âœ… RNN simple con forward pass
2. âœ… LSTM cell con todos los gates
3. âœ… LSTM completo para secuencias
4. âœ… PredicciÃ³n many-to-one
5. âœ… Comparar RNN vs LSTM

---

## ğŸ§ª Tests

```bash
npm test 14-rnn-secuencias
```

---

## ğŸ’¡ Consejos

1. **Hidden size:** 64-512 para tareas reales
2. **Secuencias largas:** Usa LSTM/GRU, no RNN simple
3. **Bidirectional:** Solo si necesitas contexto futuro
4. **Gradient clipping:** Clipear gradientes a [-5, 5]
5. **InicializaciÃ³n:** Orthogonal para Whh, Xavier para Wxh
6. **Debugging:** Imprimir shapes de tensores

---

## ğŸ“Š MatemÃ¡ticas Detalladas

**RNN Forward Pass:**
```
Para t = 1 a T:
  ht = tanh(WhhÃ—ht-1 + WxhÃ—xt + bh)
  yt = WhyÃ—ht + by
```

**Dimensiones:**
```
xt:  (input_size,)
ht:  (hidden_size,)
Whh: (hidden_size, hidden_size)
Wxh: (hidden_size, input_size)
Why: (output_size, hidden_size)
```

**LSTM Cell State Flow:**
```
Ct-1 â”€(forget)â†’ Ct â”€â†’ Ct+1
         â†‘
       (input)
```

---

## ğŸ“š Recursos

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [LSTM Paper (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)

---

**Â¡Comienza implementando RNN simple en `rnn.ts`!** ğŸ”„
