# Ejercicio 18: Reinforcement Learning

**Objetivo:** Implementar algoritmos de Reinforcement Learning (Q-Learning) y entornos simples para entender c√≥mo los agentes aprenden mediante interacci√≥n.

## üìñ Teor√≠a

### ¬øQu√© es Reinforcement Learning?

**Aprender mediante prueba y error, recibiendo recompensas o castigos.**

**Problema que resuelve:**
- No hay labels (supervisi√≥n)
- No sabemos la acci√≥n correcta a priori
- Debemos descubrir qu√© acciones maximizan recompensa a largo plazo

**Diferencia con otros ML:**
```
Supervised Learning:
  Input ‚Üí Model ‚Üí Output
  Compare con label ‚Üí Backprop

Unsupervised Learning:
  Input ‚Üí Model ‚Üí Patterns
  Sin labels, descubrir estructura

Reinforcement Learning:
  State ‚Üí Agent ‚Üí Action
  Environment ‚Üí Reward
  Maximizar recompensa acumulada
```

### Componentes de RL

**Analog√≠a: Aprender a Jugar Videojuegos**

```
Agent:    Jugador
Environment: Juego
State:    Pantalla actual (posici√≥n, enemigos, etc.)
Action:   Botones presionados (‚Üë, ‚Üì, ‚Üê, ‚Üí, A, B)
Reward:   Puntos ganados/perdidos
Policy:   Estrategia de juego
```

**Ciclo de RL:**

```
Loop:
  1. Agent observa State (s_t)
  2. Agent elige Action (a_t) seg√∫n Policy
  3. Environment ejecuta acci√≥n
  4. Environment devuelve:
     - New State (s_{t+1})
     - Reward (r_t)
  5. Agent actualiza conocimiento
  6. Repetir
```

### Markov Decision Process (MDP)

**Framework matem√°tico para RL.**

**Componentes:**
- **S:** Conjunto de estados
- **A:** Conjunto de acciones
- **P:** Probabilidad de transici√≥n P(s'|s,a)
- **R:** Funci√≥n de recompensa R(s,a,s')
- **Œ≥:** Factor de descuento [0, 1]

**Propiedad de Markov:**
```
P(s_{t+1} | s_t, a_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t, a_t)

El futuro depende solo del presente, no del pasado.
```

**Retorno (Return):**
```
G_t = r_t + Œ≥r_{t+1} + Œ≥¬≤r_{t+2} + ... = Œ£ Œ≥·µè r_{t+k}

Œ≥ = 0: Solo importa recompensa inmediata
Œ≥ = 1: Todas las recompensas valen igual
Œ≥ = 0.9: Recompensas futuras valen menos
```

### Value Functions

**V(s): Value Function**
```
V(s) = E[G_t | s_t = s]

Recompensa esperada empezando en estado s y siguiendo policy.

Ejemplo (GridWorld):
  Estado cerca de goal: V(s) alto
  Estado lejos de goal: V(s) bajo
  Estado con trampa: V(s) negativo
```

**Q(s,a): Action-Value Function**
```
Q(s,a) = E[G_t | s_t = s, a_t = a]

Recompensa esperada tomando acci√≥n a en estado s.

Ventaja: No necesitas saber P(s'|s,a)
Puedes elegir acci√≥n: argmax_a Q(s,a)
```

**Bellman Equation:**
```
V(s) = max_a [R(s,a) + Œ≥ Œ£ P(s'|s,a) V(s')]

Q(s,a) = R(s,a) + Œ≥ Œ£ P(s'|s,a) max_{a'} Q(s',a')

Relaci√≥n recursiva: Valor de estado = recompensa inmediata + valor futuro descontado
```

### Q-Learning

**Algoritmo libre de modelo (model-free).**

**No necesitas conocer:**
- P(s'|s,a): Probabilidades de transici√≥n
- R(s,a): Funci√≥n de recompensa exacta

**Solo necesitas:**
- Interactuar con environment
- Observar (s, a, r, s')

**Actualizaci√≥n Q-Learning:**

```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_{a'} Q(s',a') - Q(s,a)]
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         TD Error

Componentes:
  Œ±: Learning rate (qu√© tan r√°pido aprender)
  r: Recompensa inmediata
  Œ≥: Discount factor
  max Q(s',a'): Mejor valor futuro posible
```

**Interpretaci√≥n:**
```
Target = r + Œ≥ max Q(s',a')  "Lo que deber√≠a valer"
Error  = Target - Q(s,a)     "Cu√°nto nos equivocamos"
Update = Œ± √ó Error           "Correcci√≥n proporcional"
```

**Ejemplo paso a paso:**

```
Estado: Robot en posici√≥n (0,0)
Acci√≥n: Mover derecha
Q(0,0, right) = 0.5

Resultado:
  - Nueva posici√≥n: (1,0)
  - Recompensa: -0.1 (costo de movimiento)
  - max Q(1,0, *) = 0.8

Actualizaci√≥n con Œ±=0.1, Œ≥=0.9:
  Target = -0.1 + 0.9 √ó 0.8 = 0.62
  Error  = 0.62 - 0.5 = 0.12
  Q_new  = 0.5 + 0.1 √ó 0.12 = 0.512
```

### Exploration vs Exploitation

**Dilema fundamental de RL:**

```
Exploitation: Usar lo que sabes
  - Elegir acci√≥n con mayor Q(s,a)
  - Maximizar recompensa inmediata
  - Riesgo: Quedarse en √≥ptimo local

Exploration: Probar cosas nuevas
  - Elegir acciones aleatorias
  - Descubrir mejores estrategias
  - Riesgo: Perder recompensa a corto plazo
```

**Œµ-Greedy Policy:**

```
Con probabilidad Œµ:
  Explorar (acci√≥n aleatoria)

Con probabilidad 1-Œµ:
  Explotar (mejor acci√≥n conocida)

Decaimiento de Œµ:
  Inicio: Œµ = 1.0 (exploraci√≥n total)
  Entrenamiento: Œµ ‚Üí 0.1 (m√°s explotaci√≥n)
  Producci√≥n: Œµ = 0 (solo explotaci√≥n)
```

**Otras estrategias:**
- **Softmax:** Probabilidad proporcional a Q-values
- **UCB:** Upper Confidence Bound
- **Optimistic Initialization:** Q inicial alto

### Convergencia de Q-Learning

**Condiciones para convergencia:**

1. Todos los pares (s,a) visitados infinitas veces
2. Learning rate decae apropiadamente:
   ```
   Œ£ Œ±_t = ‚àû   (suma infinita)
   Œ£ Œ±_t¬≤ < ‚àû  (suma de cuadrados finita)

   Ejemplo: Œ±_t = 1/t cumple ambas
   ```

**En pr√°ctica:**
- Usar Œ± constante peque√±o (ej. 0.1)
- Garantizar exploraci√≥n con Œµ-greedy
- Puede no converger exactamente, pero funciona bien

### Variantes de Q-Learning

**SARSA (State-Action-Reward-State-Action)**
```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ Q(s',a') - Q(s,a)]

Diferencia con Q-Learning:
  - Q-Learning: Off-policy (usa max Q)
  - SARSA: On-policy (usa Q de acci√≥n tomada)

SARSA es m√°s conservador (aprende pol√≠tica actual)
Q-Learning es m√°s optimista (aprende pol√≠tica √≥ptima)
```

**Deep Q-Network (DQN)**
```
Q(s,a) representado por red neuronal

Mejoras:
  - Experience Replay
  - Target Network
  - Funciona con estados continuos/grandes

Usado en: Atari, Go, rob√≥tica
```

**Policy Gradient**
```
Aprende policy directamente (no Q-values)

œÄ_Œ∏(a|s) = probabilidad de acci√≥n a en estado s

Ventaja: Funciona con acciones continuas
```

---

## üéØ Escenario

**Problema:** Navegaci√≥n en GridWorld

```
Grid 5√ó5:
  S: Start
  G: Goal (+10)
  X: Obstacle (-10)
  .: Empty (-1 por paso)

‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ S ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   ‚îÇ X ‚îÇ   ‚îÇ X ‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   ‚îÇ X ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ G ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

Acciones: ‚Üë, ‚Üì, ‚Üê, ‚Üí
Objetivo: Llegar a G minimizando pasos
```

**Aprendizaje:**
```
Episodio 1: Camina aleatoriamente, cae en X
           Q-values: Aprende que X es malo

Episodio 100: Evita X, pero no encuentra G
             Q-values: Aprende √°reas seguras

Episodio 1000: Encuentra camino √≥ptimo a G
              Q-values: Convergen a valores √≥ptimos
```

---

## üìù Instrucciones

### Parte 1: Environment (GridWorld)

```typescript
export class GridWorld {
  constructor(
    width: number,
    height: number,
    obstacles?: [number, number][],
    goal?: [number, number]
  ) {
    // Inicializar grid
  }

  reset(): State;
  step(action: Action): {
    nextState: State;
    reward: number;
    done: boolean;
  };

  isValidPosition(x: number, y: number): boolean;
  render(): string;  // Visualizaci√≥n ASCII
}
```

### Parte 2: Q-Learning Agent

```typescript
export class QLearningAgent {
  constructor(
    numStates: number,
    numActions: number,
    config: QLearningConfig
  ) {
    // Inicializar Q-table
  }

  chooseAction(state: State, epsilon: number): Action;

  learn(
    state: State,
    action: Action,
    reward: number,
    nextState: State,
    done: boolean
  ): void;

  getQValue(state: State, action: Action): number;
  getBestAction(state: State): Action;
}
```

### Parte 3: Training Loop

```typescript
export function trainAgent(
  env: GridWorld,
  agent: QLearningAgent,
  config: TrainingConfig
): TrainingHistory;

export interface TrainingHistory {
  episodeRewards: number[];
  episodeLengths: number[];
  epsilonDecay: number[];
}
```

### Parte 4: Visualizaci√≥n

```typescript
export function visualizePolicy(
  env: GridWorld,
  agent: QLearningAgent
): string;

export function plotLearningCurve(
  history: TrainingHistory
): void;
```

---

## ‚úÖ Resultado Esperado

1. ‚úÖ GridWorld environment con estados y acciones
2. ‚úÖ Q-Learning agent con Q-table
3. ‚úÖ Œµ-greedy exploration
4. ‚úÖ Training loop completo
5. ‚úÖ Convergencia a pol√≠tica √≥ptima
6. ‚úÖ Visualizaci√≥n de pol√≠tica aprendida

---

## üß™ Tests

```bash
npm test 18-reinforcement-learning
```

---

## üí° Consejos

1. **Learning Rate:** Empezar con 0.1-0.5
2. **Discount Factor:** Œ≥ = 0.9-0.99 es com√∫n
3. **Exploration:** Œµ inicial = 1.0, final = 0.01
4. **Decay:** Œµ_new = Œµ_old √ó 0.995 cada episodio
5. **Episodes:** Al menos 1000 para convergencia
6. **Q-Init:** Optimistic (alto) incentiva exploraci√≥n
7. **Rewards:** Dise√±o crucial para comportamiento

---

## üìä Matem√°ticas Detalladas

**Derivaci√≥n de Q-Learning:**

```
Objetivo: Estimar Q*(s,a) √≥ptimo

Bellman Optimality:
Q*(s,a) = E[r + Œ≥ max_{a'} Q*(s',a') | s,a]

M√©todo de muestreo (sample-based):
  Observamos: (s, a, r, s')
  Estimaci√≥n: r + Œ≥ max_{a'} Q(s',a')

Temporal Difference (TD) Learning:
  Error = [r + Œ≥ max Q(s',a')] - Q(s,a)
  Update = Q(s,a) + Œ± √ó Error

Convergencia (bajo condiciones):
  lim Q(s,a) = Q*(s,a)
```

**Descomposici√≥n del Error:**

```
TD Error = Œ¥_t = r_t + Œ≥ max Q(s_{t+1}, a) - Q(s_t, a_t)

Varianza del error:
  Alta: Learning inestable
  Baja: Learning suave pero lento

Œ± controla trade-off:
  Œ± alto ‚Üí r√°pido pero inestable
  Œ± bajo ‚Üí lento pero estable
```

---

## üéÆ Extensiones

**Multi-Arm Bandit:**
- Caso especial: 1 estado, m√∫ltiples acciones
- Usado en: A/B testing, recomendaciones

**CartPole:**
- Balancear polo en carrito
- Estado continuo: posici√≥n, velocidad, √°ngulo
- Requiere discretizaci√≥n o DQN

**Atari Games:**
- Imagen como estado
- Deep Q-Network
- Convolutional layers

---

## üìö Recursos

- [Sutton & Barto: RL Book](http://incompleteideas.net/book/the-book-2nd.html)
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [David Silver's RL Course](https://www.davidsilver.uk/teaching/)
- [Q-Learning Paper](https://link.springer.com/article/10.1007/BF00992698)

---

**¬°Comienza implementando GridWorld en `reinforcement-learning.ts`!** üéÆ
